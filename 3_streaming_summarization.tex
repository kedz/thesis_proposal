\section{Content Selection Models for Streaming Summarization}
This section describes completed work on streaming summarization. We
present two approaches, one using affinity propagation (AP) clustering and 
regression \cite{some_clustering,and_regression},
and one using learning-to-search (L2S) \cite{lols}. 
This work let to two publications \cite{us1,us2} and participation in the
Temporal Sumarization track at the Text REtrieval Conference (TREC)
in 20?? and 20??. The first year, our AP clustering was the best
overall performer, and in the second year, our L2S approach was the 
??? best overall system.





In the streaming summarization task, a user supplies a query $\query$,
a brief text-based search query describing an information need, 
and time period of interest $\poi = (\tau_\alpha, \tau_\omega)$. 
The goal of the task is to sequentially process a document stream 
$X_1, X_2, \ldots$ consisting of an ordered series of
documents with corresponding timestamps $\tau_1, \tau_2, \ldots$,
extracting sentences from the document stream. We refer to the selected
sentences as updates. The document stream is ordered by timestamp where
$\tau_\alpha \le \tau_i \le \tau_\omega$ for all $i$ and $\tau_i \le \tau_j$
for all $i < j$, .


In the Temporal Summarization track, the grounding scenario was disaster
summarization, where the queries corresponeded to natural and man-made 
disasters, and the document stream consisted of online news.
Each query was associated with a set of \textit{nuggets}, textual 
descriptions of canonical pieces of important information for the query event.
See Figure~\ref{fig:events_nuggets} for an example of some of the queries and 
their nuggets. The nuggets are also assumed to have a canonical timestamp
when they are believed to have first been described by a document in 
the document stream.




\begin{figure}
    \textbf{Hurricane Shandy}\\
    ~~~~~~~~~``1986 people left without desert'' ~~ $\cdot$ ~~ ``crane brothers knocked over'' ~~ $\cdot$ ~~ ``subway is better than it will be in 2018''
    Something else \\
    something elses nuggets
    \caption{Example queries in bold, and their nuggets in quotes.}
    \label{fig:events_nuggets}
\end{figure}

Systems are rewarded when they find sentences that contain important and novel
pieces of information relevant to $\query$. Systems are penalized for 
selecting sentences that are irrelevant or contain content already covered
by previous updates. Latency penalized metrics are also computed where
the importance of a nugget decays over time. E.g. if a system
recovers the nugget ``25 people were reported injured,'' several days
after this fact was first reported, it receive less credit for it
than the system that emits that nugget an hour after it enters the 
document stream.


When considering whether or not to select a sentence for the update summary
there are two considerations. First we must estimate the salience, or 
importance, of the sentence with respect to the query. Second, we must consider
whether the relative importance of the sentence in the context of previous
updates and current candidate updates warrants selecting it for the summary.

Streaming summarization is a very hard task compared to single and 
multi-document summarization. In the latter case, the context for the 
summarization is fixed, and the input documents are usually quite 
topically focused, minimizing the prevalence of completely irrelevant 
information. In fact, in most multi-document evaluation settings, the
document collections were manually created leading to very topically
coherent text collections. 
\cite{that_guy_who_presented_once} for example found that in the ???
query focused summarization datasets are so on topic that a summarization
system could completely ignore the query and perform just as well as a
query aware system.





In the next sections, we describe to proposed solutions to this problem.
Both approaches use feature-based regression models to eestimate sentence
salience and so we briefly describe the features here.

\subsection{Features for Sentence Importance Estimation}

The streaming summarization problem is difficult precisely because the context
is constantly shifting. We cannot rely solely on word frequency because
the counts of particular ngrams will be shifting throughout the period of 
interest. Instead we compute several groups of sentence features that are
specifically helpful for the query focused task.

\paragraph{Simple Surface Features} 

We employ several basic features
that have been used previously in supervised
models to rank sentence salience (Kupiec et al.,
1995; Conroy et al., 2001). These include sentence
length, the number of capitalized words normalized
by sentence length, document position,
and number of named entities. The data stream
comprises text extracted from raw html documents;
these features help to downweight sentences
that are not content (e.g. web page titles,
links to other content) or more heavily weight important
sentences (e.g., that appear in prominent
positions such as paragraph initial or article initial).


Our most basic features look at the length in
words of a sentence, its position in the document, and the ratio
of specific named entity tags to non-named entity tokens.
We also compute the average number of sentence tokens that
match the event query words and synonyms using WordNet.


\paragraph{Query Features} Query features measure the
relationship between the sentence and the event
query and type. These include the number of
query words present in the sentence in addition to
the number of event type synonyms, hypernyms,
and hyponyms using WordNet (Miller, 1995). For
example, for event type earthquake, we match sentence
terms “quake”, “temblor”, “seism”, and “aftershock”.


\paragraph{Language Model Scores}



Similar to [Kedzie et al.,
2015], we compute the average token log probability of the
sentence on two language models: i) an event type specific
language model and ii) a general newswire language model.
The first language model is built from Wikipedia articles relevant
to the event-type domain. The second model is built
from the New York Times and Associate Press sections of the
Gigaword-5 corpus [Graff and Cieri, 2003].



Language models
allow us to measure the likelihood of producing
a sentence from a particular source. We consider
two types of language model features. The first
model is estimated from a corpus of generic news
articles (we used the 1995-2010 Associated Press
section of the Gigaword corpus (Graff and Cieri,
2003)). This model is intended to assess the general
writing quality (grammaticality, word usage)
of an input sentence and helps our model to select
sentences written in the newswire style.
The second model is estimated from text specific
to our event types. For each event type
we create a corpus of related documents using
pages and subcategories listed under a related
Wikipedia category. For example, the language
model for event type ‘earthquake’ is estimated
from Wikipedia pages under the category Category:Earthquakes.
Fig. 2 lists the event types
found in our dataset. These models are intended
to detect sentences similar to those appearing in
summaries of other events in the same category
(e.g. most earthquake summaries are likely to include
higher probability for ngrams including the
token ‘magnitude’). While we focus our system on
the language of news and disaster, we emphasize
that the use of language modeling can be an effective
feature for multi-document summarization for
other domains that have related text corpora.
We use the SRILM toolkit (Stolcke and others,
2002) to implement a 5-gram Kneser-Ney model
for both the background language model and the
event specific language models. For each sentence
we use the average token log probability under
each model as a feature.



\paragraph{Geographic Relevance} The disasters
in our corpus are all phenomena that affect some
part of the world. Where possible, we would like
to capture a sentence’s proximity to the event, i.e.
when a sentence references a location, it should be
close to the area of the disaster.
There are two challenges to using geographic
features. First, we do not know where the event is,
and second, most sentences do not contain references
to a location. We address the first issue by
extracting all locations from documents relevant to
the event at the current hour and looking up their
latitude and longitude using a publicly available
geo-location service. Since the documents that are
at least somewhat relevant to the event, we assume
in aggregate the locations should give us a rough
area of interest. The locations are clustered and
we treat the resulting cluster centers as the event
locations for the current time.
The second issue arises from the fact that the
majority of sentences in our data do not contain
explicit references to locations, i.e. a sequence of
tokens tagged as location named entities. Our intuition
is that geographic relevance is important in
the disaster domain, and we would like to take ad


\paragraph{Temporal Relevance} As we track
events over time, it is likely that the coverage of
the event may die down, only to spike back up
when there is a breaking development. Identifying
terms that are “bursty,” i.e. suddenly peaking
in usage, can help to locate novel sentences that
are part of the most recent reportage and have yet
to fall into the background.
We compute the IDF for each hour in our data
stream. For each sentence, the average TF*IDF
for the current hour t is taken as a feature. Additionally,
we use the difference in average TF*IDF
from time t to t − i for i = {1, . . . , 24} to measure
how the TF*IDF scores for the sentence have
changed over the last 24 hours, i.e. we keep the
sentence term frequencies fixed and compute the
difference in IDF. Large changes in IDF value indicate
the sentence contains bursty terms. We also
use the time (in hours) since the event started as a
feature.


\subsection{Model 1: Salience Estimation with Affinity Propagation Clustering}

  In this model, we process the document stream in hourly batches, 
  first predicting the salience of the individual sentences and then 
  using affinity propagation (AP) to select a set of exemplar sentences.
  Exemplar sentences that have predicted salience above a threshold 
  and are below a similarity threshold to previously select sentences are
  then emitted to the user as an update summary.

  
\subsubsection{Data}

  The document stream comes from the news portion of the 2014 TREC
KBA Stream Corpus \cite{frank_et_al_2012}, which contains hourly crawls
of the web covering a roughly two year span from 2011 to 2013.
Event queries and their nuggets were taken from the data prepared for 
the 2013 and 2014 TREC Temporal Summarization tracks. This data
contained 25 events and their query strings, period of interest, 
event type. 
Additionally, each event was associated with anywhere from 50 to several
hundred timestamped nugget texts. Each event query was significant 
enough to have a Wikipedia page. Event nuggets were taken manually
extracted from the corresponding Wikipedia entry, using the earliest
revision that contained the nugget to obtain it's timestamp.
For details on the creation of this dataset see \cite{aslam_et_al_2013}.

From the larger KBA Stream Corpus we created event specific document 
streams by filtering out any documents that did occur in the period
of interest and contain all the query words of the corresponding event.  



\subsubsection{Salience Estimation}

Given the feature representation of a sentence, we want to predict how
confident we are that it contains one or more nuggets. While we have
many sentences in our corpus, we did not have may sentence level judgements
about the nuggets that they contained. 
Lacking these gold annotations, we instead get noisy salience annotations 
using the maximum similarity of the nugget texts to the sentence texts.
Formally, given a query $q$, sentence text $s$, and query's nugget texts $n \in \mathcal{N}(q)$
the sentence salience $y$ is 
using the following equation:
\[ y = \max_{n \in  \mathcal{N}(q)} \operatorname{sim}(s, n)\]
where $\operatorname{sim}$ is a semantic similarity measure (in practice
we used the weight matrix factorization method of \cite{wmtf}).

We use a Gaussian process-based regression model \cite{gp} to predict $y$ from
the feature representation $\phi(s)$ \emph{without} knowledge of the nuggets.
We fit a separate regressor for each query in our dataset using 1000
randomly sampled sentences from each query's associated relevant document 
stream. We use seperate radial basis function (RBF) kernels for each 
feature group described above, and use the sum of all the kernels as final
kernel matrix for fitting the model.
At prediction time for a specific query, we hold out that query's salience
model, use the average prediction of the remaining models to obtain
a saleince estimate $\hat{y}$ for sentences in the stream.

\subsubsection{Sentence Selection with salience-biased AP}


    Affinity propagation (AP) clustering is a factor-graph based clustering
    method that simultaneously selects exemplar data points and maps 
    the remaining data points to one of the exemplars \cite{frey}. 
    The exemplar mappings determine the clusters.
    AP has a number of nice properties for extractive summarization.
    First, as an exemplar based clustering method, the cluser center
    is gaurranteed to be an actual data point observed in the input; 
    this removes the added step of selecting a representative sentence
    if we had used for $k$-means clustering, for example.
    Two, the number of clusters that result is adaptive and 
    based on the interaction of unary and pairwise factors.

    Given datapoint $n$ datapoints $X =x|^n_{i=1}$, 
    the net similarity objective
    \[ \mathcal{L}(X, \mathcal{E}) = 
    \sum_{i \in \mathcal{E}} \operatorname{salience}(x_i) + \sum_{i \in\mathcal{E}} \sum_{j:e_j = i}\operatorname{similarity}(x_i, x_j)  \]
    where $\operatorname{salience}$ and $\operatorname{similarity}$ 
    are unary and pairwise factors that express the degree to which
    $x_i$ is apriori likey to be an examplar and that $x_i$ is a suitable
    representative for $x_j$. 
    $\mathcal{L}$ is optimized using and iterative message passing 
    algorithm. In the naive setting, $\operatorname{salience}$ is uniform
    across all $x_i$, i.e. every data point is equally likely to be an
    exemplar, and exemplar assignment is purely determined by the 
    pairwise similarity factors.
    
    In our present summarization scenario, we have a strong prior belief
    about suitability of a particular sentence to be an examplar which is
    represented by the salience predictions $\hat{y}_i$.
 Our system processes hourly batches of sentences 
 $\mathcal{S}_{\textrm{batch}} = \{s_1, s_2, \ldots \}$ by first predicting
 their corresponding saliences $\hat{Y} = \{\hat{y}_1, \hat{y}_2, \ldots\}$.
 We then use AP clustering to find an assignment of exemplar sentences
 that maximizes the following objective:
 \[ \mathcal{L}(\mathcal{S}_{\textrm{batch}}, \mathcal{E}) = 
    \sum_{i \in \mathcal{E}} \hat{y}_i + \sum_{i \in\mathcal{E}} \sum_{j:e_j = i}\operatorname{similarity}(s_i, s_j)  \]
    where $\operatorname{similarity}$ is the same WTMF method used in the 
    noisy salience annotation above. We refer to this method as biased-AP
    clustering, since the exemplar selection is now biased by our prior
    beliefs about the sentence's importance to the query.


    After the examplars are selected, we perform one final filtering of
    exemplars, discarding sentences that have a $y_i$ below $\lambda_{sal}$
    or that have a maximum similarity to any previous updates above
    $\lambda_{sim}$, where the $\lambda$'s are preset thresholds. 
    Exemplars that survive filtering are selected for the update summary.



    \subsubsection{Experiments}

    Since we lack gold judgements about what sentences contain which nuggets
    we perform an automatic evaluation using ROUGE \cite{lin}. We 
    create reference summaries for each query by concatenating all 
    of it's nugget texts.
    Since there are no fixed summary lengths, and depending on the severity
    of the event, the reference summaries can vary greatly in length.
    To account for this, we report ROUGE recall, precision and F-measure.


    
    We also approximate the manual evaluation of the official TREC TS track
    by automatically mapping sentences to nuggets if their semantic similarity
    is a above a threshold. We report results across a sweep of threshold 
    values in figure ???, with values to right yielding more conservative
    estimates of performance. We report the Expected Gain and Comprehensiveness
    measures which are precission and recall like measures of nugget 
    coverage. If $N$ is the total number of nuggets for a query, $\hat{N}$ is 
    the number of unique nuggets recoverd in our 
    update 
    summary, and $U$ is the total number of sentences extracted for the update
    summary, the Expected Gain and Comprehensiveness are defined as
    \[ \frac{\hat{N}}{U} \quad \textrm{and} \quad \frac{\hat{N}}{N} \]
    respectively. An Expected Gain of 1 would mean that every sentence in
    the update summary contained a novel nugget. Comprehensiveness is the 
    nugget recall, a score of 1 indicating that all nuggets were found
    in the update summary.
    

    We refer to our approach as AP+SAL and compare to several simpler 
    baselines.
    The first is our full system but with uniform salience scores, i.e.
    the vanilla affinity propagation clustering algorithm. We refer to
    this method as AP.
    The second is to rank all sentences in each batch in order of decreasing
    predicted
    salience, and sequentially adding each sentence to the update summary, 
    omitting
    any sentences with similarity to previous updates above a threshold.
    This method is reference to as RS for rank by salience.
    Finally, we compare against another clustering algorithm,
    hierarchical agglomerative clustering.
    In this method, sentences are first clustered, 
    and then centers are determined by the sentence
    with  highest  cosine  similariy  to  the  cluster
    mean. 
    Sentences are added to the update summary in time order, 
    removing sentences that are highly similary to previous updates in the
    same manor as the RS method. We refer to this method as HAC.

    \subsubsection{Results}


Table  ???  shows  our  results  for  system  output
samples against the full summary of nuggets using ROUGE. This improvement is statistically significant  for  all  ngram  
precision,  recall,  and  F-measures at the
$\alpha=.01$
level using the Wilcoxon
signed-rank test.
AP+SALIENCE
maintains    its    performance
above  the  baselines  over  time  as  well.
Figure  3  shows  the  ROUGE-1  scores  over  time.
We  show  the  difference  in  unigram  precision
(bigram  precision  is  not  shown  but  it  follows
similar  curve).
Within  the  initial  days  of  the
event,  AP+SALIENCE
is  able  to  take  the  lead
over  the  over  systems  in  ngram  precision.   The
AP+SALIENCE
model is better able to find salient
updates earlier on; for the disaster domain, this is
an especially important quality of the model.
Moreover, the AP+SALIENCE’s recall is not diminished by the high precision and remains com-
petitive with AP. Over time AP+SALIENCE's recall also begins to pull away, while the other mod-
els start to suffer from topic drift.


Figure ??? shows the expected gain across a range
of  similarity  thresholds,  where  thresholds  closer
o 1 are more conservative estimates. The ranking
of the systems remains constant across the sweep
with AP+SALIENCE
beating all baseline systems.
Predicting salience in general is helpful for keeping a summary on topic as the  RS  approach out
performs  the  clustering  only  approaches  on  expected gain.
When looking at the comprehensiveness of the
summaries AP outperforms AP+SALIENCE.  
The compromise  encoded  in  the  AP+SALIENCE
objective function, between being representative and
being salient, is seen clearly here where the per-
formance of the AP+SALIENCE
methods is lower
bounded by the salience focused  RS  system and
upper bounded by the clustering only AP system.
Overall, AP+SALIENCE
achieves the best balance
of these two metrics.


\subsection{Model 2: LOLS}

Our next model attempts to address some of the shortcomings of the 
first. First the previous model process sentences in hourly batches.
Ideally we would process new documents as soon as possible in order 
to minimize the negative effects of latency. Second, our salience
regressors were statically trained and did not take advantage of features
like the similarity to recently selected updates and relied mostly on
tuned thresholds to account for redundancy. 

Getting around these limitations poses some severe challenges. Using
dynamic features means that naive traing will require gold reference
sentence extracts and that naively training a sentence salience model
with these will under explore the space of plausable update summaries,
meaning that in practice errors are likely to snowball over time.

Instead, we develop a clairvoyant oracle summarizer $\pi^o$ whose behavior
we want to imitate. $\pi^o$ has knowledge of what nuggets, if any, 
are present in each sentence in the document stream, and it's behavior
is quite simple: it processes each sentence in a query's relevant document 
stream and 
when it encounters a sentence with a novel nugget, it adds that sentence
to the update summary.

As we stated before, training only on a single oracle run over document stream
would be sub-optimal because the oracle is perfect and it would finish 
recovering all of the nuggets quite quickly and then do nothing for
the remainder of the stream. In practice, our learned model is likely to
make mistakes, missing the first few appearances of a nugget but hopefully
recovering them as repetition in the stream makes them more likely to be 
selected. Only following the oracle's first best pass would not help us learn
to recover from errors.

To make better use of the oracle, we adopt the locally optimal learning to
search (LOLS) algorithm \cite{lols}, one of a family of learning to search 
(L2S) algorithms. In order to formally describe the algorithm, we first 
introduce some notation. We treat the streaming summarization problem
as a markov decision process. At each timestep $t$ we observe a state
$s_t \in \mathcal{S}$ and $t$-th sentence $x_t \in \mathcal{X}$ from the 
document stream.
A policy $\pi : \mathcal{S} \times \mathcal{X} \rightarrow \{0, 1\}$
maps a state-sentence tuple to an action $a \in \mathcal{A} =\{0,1\}$
where $a=1$ indicates we extract the sentence and $a=0$ indicates we skip
the current sentence. The transition function 
$d : \mathcal{S} \times \mathcal{X} \times \mathcal{A} \rightarrow \mathcal{S}$
deterministically maps state-sentence-action tuples to the next state. 
In practice the state contains set of sentences previously extracted and
other rolling statistics from previously observed sentences in the stream.
Our training objective is to minimize the expected costs of each action:
\[ \mathcal{L}(\pi) = \mathbb{E}_{s,x \sim \pi} \left[ c\left(\pi(s,x), s, x\right) \right] \]
where $c :\mathcal{A} \times \mathcal{S} \times \mathcal{X} \rightarrow \mathcal{A}$ is the cost of taking a given action in a particular state-sentence
observation.

~\\

~\\

In the LOLS training regime, there are two phases: the 
\emph{roll-in} and the \emph{roll-out}. The roll-in phase 

at each timestep 
we alternate between using the oracle $pi^o$ or our current learned policy
$\hat{\pi}$ to run multiple time steps into the future and get a cost for 
a hypothetical summary we would have created in the case where we extracted the 
current sentence or skipped it. We collect these scores for each decision
and update a regression model to accuractely predict the cost of each action
given the current sentence and update summary/stream state.





\subsubsection{Data}
We ran 

\subsubsection{Oracle Policy and Loss Function}

We use a greedy oracle that selects sentences that contain novel
nuggets. This oracle will achieve an optimal Comprehensiveness score, i.e.
it will obtain every possible novel nugget in the roll-out phase.
However, it will not always achieve the maximum possible Expected Gain.
For example, consider the sequence of sentences $s_1, s_2, s_3$, where
nugget $n_1 \in s_1$, $n_2 \in s_2$, and $n_1,n_2 \in s_3$. The greedy oracle,
proceeding sequentially would select sentences $s_1$ and $s_2$, and skip
$s_3$, achieving an Expected Gain of $\frac{|\{n_1, n_2\}|}{|\{s_1, s_2 \}|} = \frac{2}{2} = 1$. The maximum achievable Expected Gain is obtained by skipping
the first two sentences and selecting sentence $s_3$ yielding $\frac{|\{n_1, n_2\}|}{|\{s_3 \}|} = \frac{2}{1} = 2$. In practice we are far from matching
the greedy oracle and so it suffices for now as an aspirational target.



We used the complement of Dice coefficient as the loss function.



\subsubsection{Experiments }

In our results, we refer to our learn-to-search approach as LS.
We compare to a lead sentence baseline that takes the first sentence
of every document as an update if it's maximum cosine similarity to any 
previous update was below a threshold. We refer to this method as COS.

We also compare our 








suboptimal in that it will achieve a perfect
Comprehensiveness 










%During the a tim

%While  An
%information retrieval (IR) system will then begin 














\section{Introduction}

Everyday we ask friends and colleagues to do it, to tell us about books, 
newspaper articles, and other complex texts, and, without more than a 
moment's reflection, they are able to compress their experience
into succinct natural language statements that describe the
subject of our request.
The ease with which we summarize belies the difficulty of
writing programs that do so, and so it would appear that 
the robust
ability to create summaries is, as of 2018, a uniquely human capability.
This partially explains summarization's allure to the artificial intelligence (AI)
research community, which has in one way or another, attempted to aggregate
and naturally compress text data since at least the 1950's 
\citep{luhn1958automatic}.
This makes automatic text summarization one of the longest-standing 
application areas in the 
field of natural language processing (NLP), with a history as deep 
as some of the more foundational tasks, e.g. syntactic parsing 
\citep{yngve1955syntax} or 
part-of-speech tagging \citep{harris1962string}. 

While there are many variants, 
the general summarization task is 
to reduce a large input text into its most essential pieces of information,
and in doing so reduce the amount of reading a human has to do. 
In this thesis we focus on advances to two summarization subtasks:
\textit{(i)} identifying the most import content for inclusion in the summary, 
and \textit{(ii)}
rendering that content in such a way as to not misrepresent the original 
input. We refer to the former as \textbfit{salience estimation} and the latter
as \textbfit{faithful generation}. 

\paragraph{Salience Estimation} Our completed and proposed methods of salience estimation cover a range of
techniques including regression, clustering, learning-to-search,
and deep neural networks. 
We experimentally verify the utility of our approaches
across a variety of summarization tasks including stream summarization,
single-document summarization, and multi-document summarization.
The general research goal here is to develop flexible models for 
identifying
important words, phrases, and sentences that are likely to serve as 
representative members of the larger text in which they are found.

In extractive summarization, 
where the summary is constructed by copying and pasting phrases or sentences 
from the input text, salience estimation can be used directly to create
a summary. For example, a simple method of sentence extractive summarization
would be to order the input sentences by decreasing salience and select the
top few sentences for the summary. In most cases, salience is not the only
consideration for summary inclusion; redundancy, discourse coherence, fluency,
and many other metrics of text quality can and have been used as content
selection criteria. These measures are largely independent of salience 
(with the notable exception of redundancy) and so we do not explore them in 
detail here. The details of our completed and proposed work on 
salience estimation can be found
in sections \ref{sec:feature_salience} and \ref{sec:deep_learning_salience}.

In \autoref{sec:feature_salience} we develop two feature-based models of 
sentence
salience and evaluate them in a relatively novel stream 
summarization crisis-monitoring scenario
\citep{starbird2013working,aslam2015trec,aslam2016trec}. In this task, a user is interested
in tracking information about a disaster event (e.g. a hurricane). The
user provides a text based query describing the event 
(e.g. ``Hurricane Sandy'') to the summarization model. 
The model must then 
monitor a stream of news articles, extracting important sentences in the 
document stream and present them to the user. The information must be 
highly salient to the query event, as well as novel and timely to the user 
-- we do not want to bombard the user with irrelevant or repeated information,
and our methods cannot take so long  that the extracted
information is out of date.
  
We develop and compare several deep learning based models of word and 
sentence salience in \autoref{sec:deep_learning_salience} and evaluate them 
primarily on
sentence extractive single document summarization (SDS). 
We perform this evaluation across
a variety of genres including news, personal narratives, and medical journal
articles. Intuitively, the goal here is to summarize a single news article, 
short story, or research paper by selecting a subset of the input sentences
to serve as the summary. We also explore adaptation of these algorithms to 
the sentence extractive multi-document summarization (MDS) task, 
where there is much less training data.
In this task, an extractive summarization model is given a set of related 
documents 
(typically ten), and must select a subset of the sentences to use as the summary of
the document set.

\paragraph{Faithful Generation} Finally, section~\ref{sec:faithful_generation} describes our proposed 
contributions to
abstractive summarization, i.e. methods that produce novel 
summary text using a generative model. 
Research in abstractive summarization is increasing, in part due to
the success of general purpose sequence-to-sequence transduction models 
for machine translation (MT) that have been ported to the summarization task.
The ability of abstractive models to generate fluent 
summaries is impressive. However, they are also especially prone 
to generating generic statements and hallucinated facts that are not grounded 
by evidence in the input. %This seems to be a well known problem among 
%researchers working
%in generation, but it has not received much direct attention in the literature.
We are interested in this problem from both the perspectives of selecting
the right evidence in the input for generation, a task for which we enlist
our salience estimation methods, and also certifying that a generated text
conforms to knowledge represented in the input or possessed by the model
\textit{a priori}. For the latter goal, we propose to model generation as a two player
game, where one player, the generator, is tasked with producing a text
utterance describing a piece of evidence (either text or table data);
the second player, the recognizer, %change this name
evaluates the plausibility of the utterance with respect to the evidence.
This overall regime will constitute our proposed method called
 faithful generation.
%to be outlined in more detail in \autoref{sec:faithful_generation}. 
We propose 
evaluation both on an abstractive SDS task \citep{volske2017tl}, 
as well as, two data-to-text generation 
test beds \citep{lebret2016neural,novikova2017e2e}.





%Salience estimation can also be used to guide \textbfit{abstractive 
%summarization} techniques, i.e. methods that produce novel
%summary text using a generative model. These types of models 
%must (at the very least) implicitly select important content for conditioning 
%their
%generative process. We propose a salience estimation component to 
%explicitly model this selection process in addition to a generative model 
%training regime which we briefly describe presently. (NOTE-TO-SELF: reorder this transition you dumb dumb.)
%
%Research in abstractive summarization is increasing, in part due to
%the success of general purpose sequence-to-sequence transduction models 
%in machine translation (MT) that have been ported to the summarization task.
%The ability of abstractive models to generate relatively fluent and 
%meaningful summaries is impressive. However, they are also especially prone 
%to generating generic statements that are not grounded by evidence in the 
%input. This seems to be a well known problem among researchers working
%in generation, but it has not received much attention in the literature.
%We are interested in this problem from both the perspectives of selecting
%the right evidence in the input for generation, a task for which we enlist
%our salience estimation methods, and also certifying that a generated text
%conforms to knowledge represented in the input or possesed by the model
%apriori. For the latter goal, we propose to model generation as a two player
%game, where one player, the generator, is tasked with producing a text
%utterance describing a piece of evidence (either text or structured data);
%the second player, the recognizer, %change this name
%evaluates the plausibility of the utterance with respect to the evidence.
%This overall regime will consitute our method of faithful generation,
%to be outlined in more detail in \autoref{sec:faithful_generation}. We propose 
%evaluation both on an SDS task, as well as, two data-to-text generation 
%test beds \citep{lebret2016neural,novikova2017e2e}.

\subsection{Contributions}

To summarize, our completed and proposed contributions of this thesis are as 
follows:
\begin{enumerate}

 \item Two novel feature-based models of sentence salience and an empirical
    evaluation on a stream summarization task.
% \item A novel approach to streaming summarization using a feature-based
%     regression model of sentence salience and sentence selection using 
%     exemplar-based clustering.

% \item A novel approach to streaming summarization using the locally optimal
%     learning to search (LOLS) algorithm.

 \item Several novel deep learning architectures for word and sentence 
   salience, a thorough evaluation of the linguistic and
   structural features critical to learning in the SDS task across a 
   variety of genres, and adaptation to a news MDS task. 


 \item A novel training regime for generative models of text, 
     called faithful generation, to ensure that the generated text does
     not misrepresent the conditioning input. We develop faithful generation
     models for both the data-to-text and text-to-text (i.e. summarization)
     settings.

 \item A novel method of combining salience estimation based extractive 
     summarization with abstractive generation in the faithful generation
    paradigm.    

%An experimental evaluation of several existing and novel deep learning
%   architectures for word and sentence salience with 
%A study on the design, strengths, and limitations of deep learning 
%     models for content selection at the sentence and word level in 
%     single document summarization. 

% \item An experimental study of extractive content selection algorithms
%     as input to abstractive summarization model.

% \item A novel approach to generating text that is faithful to some
%     structured information in a database.

\end{enumerate}




%The first two contributions focus broadly on estimating content importance
%or salience for various summarization tasks and domains. They further
%can be broken down into traditional feature-based models (the first two 
%contributions) and deep learning models (the third contribution).
%The last two contributions focus on generating text from previously 
%selected content, with the fourth contribution studying the case where
%selected content consists of extracted sentences and words from an input
%document, and the final contribution focuses on generating text that is 
%faithful to some structured information. 

Together, these contributions
provide a wide array of experiments and  methodology for identifying summary worthy information and 
generating text that respects truth statements about that information.
The hope is that these methods will lead to more reliable summaries
without sacrificing the expressiveness of the generation algorithm. 
In the next section (\autoref{sec:feature_salience}) 
we describe completed work on feature-based approaches to sentence salience estimation. 
In section~\ref{sec:deep_learning_salience}, we describe completed and ongoing
 work on deep neural network models
of salience estimation. We describe our planned approaches to faithful 
generation in section \ref{sec:faithful_generation}. 
Section~\ref{sec:research_plan} describes our research plan, before 
we finally conclude.







\subsection{Deep Learning Models of Sentence Salience}

 Given the diversity of neural architectural choices, a best practices
for sentence extractive summarization has yet to emerge. In this section
we ask what architecture design choices matter for single document 
summarization across a variety of domains.

We begin by defining some terminology. A sentence is represented as an
sequence of words $\sent = \word_1, \word_2, \ldots, \word_{|\sent|},$
where each word is drawn from a finite vocabulary $\vocab$ and $|\sent|$ is
the length of sentence $\sent$ in words. Similarly, a document $\doc =  
\sent_1, \sent_2, \ldots, \sent_{|\doc|} $ is a sequence of sentences, where 
$|\doc|$ is the size of the document in sentences.  

We treat sentence extractive summarization as a sequence tagging
problem: given a document $\doc$, we want to assign an associated binary
tag sequence $\Labels \in \{0,1\}^{\Sze{\doc}}$ such that the corresponding
set of extracts $\extracts = \{\sent_i \in \doc \; | \; \Labels_i =1 \}$ is
a suitable summary
of the document. Typically, it is assumed that the size of the extract summary
in sentences is much smaller than the input document, i.e. 
$|\extracts|  \ll |\doc|$.
It is also common to enforce a word budget $\budget$ such that
$\sum_{\sent \in \extracts} |\sent| \le \budget$.

A typical deep learning model will build up a hierarchical representation
of each sentence, starting at the word level, and then composing an arbitrarily
long sequence of word representations into a fixed length sentence 
representation.
First the individual words are 
projected to fixed length vectors, or word embeddings. %, via a mapping
%$\embproj : \vocab
%\rightarrow \mathbb{R}^{\embsize}$. 
The sentence encoder network %$\encoder :
%\{\mathbb{R}^{\embsize}\}^* \rightarrow \mathbb{R}^{\sembsize}$ 
is then 
responsible for mapping word embedding sequences to fixed length sentence 
embeddings. Finally, the sentence extractor network %$\extractor : 
%\{\mathbb{R}^\sembsize\}^* \rightarrow \{0, 1\}^*$ 
produces a label 
sequence $\Labels$.

We explore several choices of encoder and extractor architecture from the 
literature \citep{cheng2016neural,nallapati2016summarunner} as well as 
propose our own designs \citep{kedzie2018deep}. For complete details of 
the different model architectures please see our paper. In the next sections,
we describe the different encoder and extractor architectures, before 
discussing data, experiments, and evaluation. 

%the three sentence encoder architectures (\enAvg, 
%\enRnn, and \enCnn) followed by four extractor architectures 
%(\textit{rnn}, \textit{seq2seq}, \textit{sr}, and \textit{cl}).

\subsubsection{Sentence Encoders}
We explore three different sentence encoders commonly used in the literature.
\begin{enumerate}
    \item \textbf{Averaging Encoder}~~A sentence embedding is obtained by
        averaging its associated word embeddings. 
    \item \textbf{RNN Encoder} We run a bidirectional RNN over the sentence's
        word embeddings, taking a concatenation of the final forward 
        and backward output states as the sentence embedding.
        We use a gated
        recurrent unit \citep{cho2014learning} as the actual RNN cell.
    \item \textbf{CNN Encoder} We use a CNN over a sentence's word embedding
        to extract \ngram{} features. Our implementation is similar to 
        \cite{kim2014convolutional}, i.e.  one dimensional convolutions of 
        varying width \ngram{} feature detectors with max pooling.
\end{enumerate}



%\paragraph{Averaging} This encoder simply averages a sentence's
%associated word embeddings:
%\[ \encoder_{avg}(\sent) = \frac{1}{|\sent|} \sum_{\word \in \sent} \embproj(\word). \]
%Other than the word embeddings, this encoder involves no learned parameters,
%and while it collapses word order, embedding averaging has consistently
%been found competitive with more sophisticated sentence embedding techniques
%\citep{iyyer2015deep,wieting2015towards,arora2016simple,wieting2017revisiting}.
%
%
%\paragraph{Recurrent Neural Networks} The second encoder architecture
%we experiment with is a recurrent neural network (RNN) over the word
%embeddings. An RNN maintains an internal ``hidden'' state that is sequentially
%updated upon observing each word embedding. In practice, we use a bidirectional
%RNN with a gated recurrent unit (GRU) as the particular instantiation of 
%the RNN cell \citep{cho2014learning}.
%Under the RNN encoder, a sentence embedding for a sentence $\sent$ is defined 
%as the concatenation of the final forward and backward GRU outputs.
%
%\paragraph{Convolutional Neural Networks}
%Our final sentence encoder uses a convolutional neural network
%(CNN) to encode salient n-gram windows into a fixed length vector. 
%CNN's have grown increasingly popular in many NLP tasks 
%as a computationally efficient substitute for RNN-based architectures
%\citep{kim2014convolutional,lei2015molding,dauphin2017language}.
%Our architecture largely follows \cite{kim2014convolutional}.


\subsubsection{Sentence Extractors}

A sentence extractor takes the encoder output, i.e. 
a sequence of sentence embeddings
$\encoder(\doc) =$ $\encoder(\sent_1), \ldots, \encoder(\sent_{|\doc|}) =
\semb_1, \ldots, \semb_{|\doc|}$, and produces an extract label sequence
$\Labels$. 
The sentence extractor is essentially a discriminative
classifier $p(\Labels_{1:\Sze{\doc}} | \semb_{1:\Sze{\doc}})$.
Prior neural network approaches 
\citep{nallapati2016summarunner,cheng2016neural} 
to sentence extraction have assumed
an auto-regressive model, leading to a semi-Markovian
factorization of the extractor probabilities
$p(\Labels_{1:\Sze{\doc}} | \semb_{1:\Sze{\doc}})=\prod_{i=1}^{|\doc|} 
p(\Labels_i|\Labels_{<i},\semb_{1:\Sze{\doc}})$,
where each prediction $\Labels_i$ is dependent on \emph{all}
previous $\Labels_j$ for
all $j < i$. We compare two such models proposed by \cite{cheng2016neural}
and \cite{nallapati2016summarunner}.
A simpler approach that does not allow interaction among the $\Labels_{1:n}$
is to
model $p(\Labels_{1:\Sze{\doc}}|\semb_{1:\Sze{\doc}}) = 
\prod_{i=1}^{\Sze{\doc}} p(\Labels_i|\semb_{1:\Sze{\doc}})$,
which we explore in two proposed extractor models.
Overall, we experiment with four different sentence extractor architectures
which we now describe briefly.

\begin{enumerate}

    \item \textbf{SummaRunner Extractor \citep{nallapati2016summarunner}} 
      The SummaRunner extractor is built 
      around an RNN over sentence embeddings along 
      with separate components to estimate contributions of document 
      similarity, iterative summary similarity, and coarse and fine grained 
      sentence position. Any one extraction decision $\Labels_i$ is 
      dependent on all previously label decisions $\Labels_{<i}$.
  \item \textbf{RNN Extractor (ours)} We propose a stripped down version
      of SummaRunner using only an RNN over sentence embeddings 
      without any of the  representations of document, summary,
      or position. The RNN output is fed to a sigmoid layer to predict the
      probability of extraction. All sentence extraction decisions are
      independent once the hidden layers of the RNN have been computed. 
  \item \textbf{Cheng \& Lapata Extractor \citep{cheng2016neural}}
      This extractor is built around a sequence-to-sequence model with 
      distinct encoder and decoder RNNs over sentence embeddings (the encoder
      here is distinct from the sentence encoder over words).
      In effect the model processes the document twice. Predictions are 
      made using the previous timestep's decoder output, and the current 
      timestep's encoder output. Decoder inputs are weighted by the previous
      extraction probability, inducing a similar dependence structure 
      to the SummaRunner model where any extraction decision $\Labels_i$
      is dependent on all previous decsions $\Labels_{<i}$.
 \item \textbf{Seq2Seq Extractor (ours)} Our simplified version of the 
     Cheng \& Lapata extractor is a standard sequence-to-sequence model
     \citep{bahdanau2014neural} with dot-product style attention
     \citep{luong2015effective}. A concatenation of the decoder output and 
     attention context are fed into a sigmoid layer to make predictions.
     Like the RNN extractors, sentence extraction decisions are
      independent once the RNN and attention layers have been computed.
\end{enumerate}

%that we refer to as the RNN
%and Seq2Seq extractors.

%\paragraph{SummaRunner Extractor}\citet{nallapati2016summarunner} proposed
%a sentence extractor, which we refer to as the SummaRunner Extractor,
%that factorizes the extraction probability into contributions 
%from different sources.
%First, a bidirectional RNN is run over the sentence embeddings\footnote{\citet{nallapati2016summarunner}
%    use an RNN sentence encoder with 
%this extractor architecture; in this work we pair the SummaRunner extractor
%with different encoders. } and the output is
%concatenated. A representation of the whole document is made by 
%averaging the RNN output. A summary representation is also constructed 
%by taking the sum of the previous RNN outputs weighted by their extraction
%probabilities. Extraction predictions are made using 
%the RNN output at the $i$-th prediction step, the document representation, and 
%$i$-th version of the summary representation, along with factors for 
%sentence location in the document. The use of the iteratively constructed
%summary representation creates a dependence of $\Labels_i$ on all 
%$\Labels_{<i}$.
%
%
%
%
%\paragraph{RNN Extractor \citep{kedzie2018deep}}
%    Our first proposed model is a very simple bidirectional
%RNN based tagging model, and can be thought of as a 
%bare bones version of the SummaRunner
%extractor. In this model, we pass the outputs of a bidirectional GRU over the 
%sentence embeddings into a multi-layer perceptron that terminates in a layer 
%of
%sigmoid activations corresponding to the sentence level extraction probabilities.
%%sentence encoder we use a GRU cell.
%%The forward and backward outputs of each sentence are passed through a 
%%multi-layer perceptron with a logsitic sigmoid output 
%%to predict the probability
%%of extracting each sentence. 
%%See \autoref{fig:extractors}.a for a graphical layout.
%%and \autoref{app:rnnextractor} for details.
%
%
%%\newcommand{\rExtHidden}{\overrightarrow{h}}
%%\newcommand{\lExtHidden}{\overrightarrow{h}}
%%\newcommand{\docSize}{|\doc|}
%%\newcommand{\logits}{o}
%
%
%\paragraph{Cheng \& Lapata Extractor} 
% This extractor, proposed by 
%\citet{cheng2016neural}, %, which we refer to as the Cheng \& Lapata Extractor,
%is built around a sequence-to-sequence model.
%First, each sentence embedding\footnote{\citet{cheng2016neural} used an CNN sentence encoder with 
%this extractor architecture; in this work we pair the Cheng \& Lapata extractor
%with several different encoders.} is
%fed into an encoder side RNN, with the final encoder state passed to the
%first step of the decoder RNN. On the decoder side, the same sentence 
%embeddings are fed as input to the decoder and both encoder and decoder 
%outputs are used to predict each $\Labels_i$. The decoder input is weighted by the previous 
%extraction
%probability, inducing the dependence of $\Labels_i$ on $\Labels_{<i}$.
%
%
%
%
%\paragraph{Seq2Seq Extractor \citep{kedzie2018deep}} 
%%One shortcoming of the RNN extractor is that long range
%%information from one end of the document may not easily be able to affect 
%%extraction probabilities of sentences at the other end. 
%Our second proposed model, the represents a more typical sequence-to-sequence
%architecture with dot product style attention \citep{luong2015effective},
%%mitigates this problem with an 
%%attention 
%%mechanism commonly
%used for neural machine translation \cite{bahdanau2014neural} and 
%abstractive summarization \cite{see2017get}. 
%The sentence embeddings are first
%encoded by a bidirectional $\gru$. A separate decoder $\gru$ transforms each 
%sentence into a query vector which attends to the encoder output. The
%attention weighted encoder output and the decoder $\gru$ output are concatenated
%and fed into a multi-layer perceptron to compute the extraction probability.
%%See \autoref{fig:extractors}.b for a graphical layout.
%Unlike the Cheng \& Lapata extractor, the Seq2Seq extractor does not induce 
%dependencies between the extraction labels, i.e. they are conditionally independent.
%
\subsubsection{Data}
\input{dl_based_salience_models/figures/datasets.tex}
We perform our experiments across six corpora from varying domains to 
understand how different biases within each domain can affect content 
selection. The corpora come from the news domain
(CNN-DailyMail, New York Times, DUC), personal narratives domain (Reddit),
workplace meetings (AMI), and medical journal articles (PubMed). See 
\autoref{tab:dldata} for dataset statistics and \cite{kedzie2018deep} for a
complete description of the preprocessing.
The news datasets are fairly common in the summarization literature and
allow us to consider performance across large, medium, and small dataset
sizes. The PubMed dataset contains similarly formal and position dependent
text as newswire, and it allows us to examine the effect of these features
in another genre.
The Reddit and AMI corpus are less dependent on sentence position
than the
other datasets, however their smaller size makes them less useful as 
datasets for deep learning models. The AMI corpus is also transcribed speech
which makes it a fairly unique summarization corpus.

%\paragraph{CNN-DailyMail} We use the preprocessing and training, validation, 
%and test splits
%of \cite{see2017get}.
%This corpus is a mix of news on different topics including politics,
%sports, and entertainment.
%
%\paragraph{New York Times}The New York Times (NYT) corpus \citep{sandhaus2008new} contains
% two types of abstracts for a subset of its articles. The first summary is
%an archival abstract and the 
%second is a shorter online teaser meant to entice a viewer of the webpage to
%click to read more. From this collection, we take all articles that have 
%a concatenated summary length of at least 100 words.
%We create training, validation, and test splits by partitioning on dates;
%we use the year 2005 as the validation data, with training and test partitions
%including documents before and after 2005 respectively.
%
%\paragraph{DUC} We use the single document summarization data from the 2001
%and 2002
%Document Understanding Conferences (DUC) \citep{over2002introduction}. We split the 2001 data into training
%and validation splits and reserve the 2002 data for testing.
%
%\paragraph{AMI} The AMI corpus \citep{carletta2005ami} 
%is a collection of real and staged office meetings
%annotated with text transcriptions, along with abstractive
%summaries. We use the prescribed splits. 
%
%\paragraph{Reddit} \citet{ouyang2017crowd} collected a corpus of personal 
%    stories shared
% on Reddit\footnote{\url{www.reddit.com}} along with multiple extractive 
% and abstractive summaries. We randomly split this data using roughly three and five percent of the data validation and test respectively.
%
%\paragraph{PubMed}{We created a corpus of 25,000 randomly sampled
%    medical journal articles from the PubMed Open Access 
%    Subset\footnote{\url{https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/}}.
%    We only included articles if they were at least 1000 words long and 
%    had an abstract of at least 50 words in length.
%We used the article abstracts as the ground truth human summaries.}

\paragraph{Ground Truth Extract Summaries}
Typically, the reference summaries in our datasets are human written
abstracts and therefore we cannot use them directly to learn a sentence 
extractive summarization model.
Instead, we use them to construct gold label sequences
%Since we do not typically have ground truth extract summaries from which to
%create the labels $\Labels_i$, we construct gold label sequences 
by greedily optimizing \rougeN{1} with respect to the reference abstracts
as in \cite{nallapati2016summarunner}.
We choose to optimize for \rougeN{1} rather than 
\rougeN{2} similarly to other optimization based approaches to summarization 
\citep{sipos2012large,durrett2016learning,nallapati2016summarunner} 
which found this to be the easier target to learn.






\subsubsection{Experiments}

 We are interested in two questions. The first, more pragmatic question, is
 what are the best configuration of encoder/extractor architectures?
 We answer this by evaluating \rouge{} recall
performance across our six collected datasets.
 We also used \meteor{} 
\citep{denkowski:lavie:meteor-wmt:2014} but omit these numbers here
for space since they
show the same trends; see \cite{kedzie2018deep} for the complete results.
  We perform the standard
 stochastic gradient descent based optimization (using the Adam
 update \citep{kingma2014adam}) of the weighted negative log likelihood 
 of the gold extract labels to fit model parameters. We upweight the loss for positive labels (i.e.
 extractions) since they are relatively sparse compared to the negative 
 labels.
% \[ \mathcal{L}(\theta) = -\sum_{\doc, \Labels \in \mathcal{D}} 
%            \sum_i \omega(\Labels_i) 
%        \log p(\Labels_i|\Labels_{<i}, \encoder(\doc); \theta) \]
   %     where $\theta$ are model parameters and $\omega(\Labels_i)$ upweights
   %     the positive labels to account for the imbalanced label distribution.

% We lack human reference extract labels for our datasets and so we obtain
% said lable sequences heuristically, by finding a label sequence $\Labels^*$
% by greedily optimizing ROUGE-1 recall with respect to the human reference
% abstracts.

 The second question, is more diagnostic in nature: what signals
 in the data are driving model learning?
 We perform several experiments to find answers. 
 We hypothesize that the lexical semantics encoded at the word embedding
 level will be important to subsequent sentence representations, and
 perform a comparison on learning with and without fine tuning of the 
 embeddings. In both cases, embeddings are initialized with \textsc{Glove}
 embeddings pretrained on Wikipedia and Gigaword \citep{pennington2014glove}.
 
 
 We also hypothesize that certain classes of words will be more important 
 to identifying salient content than others. We perform word ablation 
 experiments where we alternately remove nouns, verbs, adjectives \& adverbs,
 and function words from the sentence encoder input and compare performance 
 to the non-ablated system. We expect that the nouns will be more important
 to content selection. 


 Our final experiments attempt to tease out the effect of structural features 
 from the lexical. In this experiment, we shuffle the sentence order at 
 training time. In this setup, we obfuscate features about which content 
 was introduced in the article first, an important and well known bias in the
 news domain \citep{nenkova2005automatic}. 

 

 \subsubsection{Results}

 \input{dl_based_salience_models/figures/table_overall_results.tex}
  

The results of our main experiment comparing 
the different extractors/encoders are shown in 
Table~\ref{tab:results} (Overall Results).
Overall, we find no major advantage when using the CNN and RNN sentence
encoders over the averaging encoder (see \textcolor{red}{red} Seq2Seq Avg. row). The best performing encoder/extractor pair either 
uses the averaging 
encoder (five out of six datasets) or the differences 
are not statistically significant. %When only comparing within the 
%same extractor choice,  the averaging encoder is the better choice
%in 14 of 20 cases. 
%\hal{i wonder if it would be worth adding another ``average performance metric'' column to \autoref{tab:results}.
%  i'm thinking have ``Average $\Delta$-Best'' meaning how far (on average across the datasets) is this setting from the best setting available on that dataset.
%  so since the best numbers are: 25.56, 35.85, 23.11, 13.65, 5.63
%  and the first row numbers are: 25.42, 34.67, 22.65, 11.37, 5.50
%  then the deltas are:            0.14,  1.18,  0.46,  2.28, 0.13
%  the the average delta is 0.84 (assuming my math is right)
%  there's an argument to do multiplicative, in which case
%  the multipliers for first row:  0.99,  0.97,  0.98,  0.83, 0.97
%  and the average is 0.95
%  either way this gives a quick way to make comparisons between rows. you could do the same for the other tables too.}

  %\hal{in some of the tables you list R-2 as headers even though all the numbers are R-2. just put that in the caption.}


When looking at extractors, the Seq2Seq extractor is either part of 
the best performing system (three out of six datasets) or is not 
statistically distinguishable from the best extractor (see again 
\textcolor{red}{red} Seq2Seq Avg. row). 

Overall, on the news and medical journal domains, the differences are 
quite small with the 
differences between worst and best systems on the CNN/DM dataset 
spanning only .56 of a \rouge{} point. While there is more performance variability
 in the Reddit and AMI data, there is less distinction among systems: 
 no differences are significant on Reddit
and every extractor has at least one encoder configuration that is indistinguishable
from the best system on the AMI corpus. This is probably due to the small test
size of these datasets.
%\hal{this is probably at least partially because of test set size. maybe mention this.}





%?\textcolor{red}{Overall we find that the \modelTwoBF~extractor achieves the 
%?best ROUGE scores on three out of four domains (STILL RUNNING ON AMI AND PUBMED). 
%?However, most
%?differences are not signficant. (Need to discuss stat sig and how to show it).}
%?On the larger CNN-DailyMail dataset, especially, 
%?differences are quite smail across all extractor/encoder pairs.
%?The \baselineOneBF~extractor achieves the best performance on the DUC 2002
%?dataset. It is disappointing that the \baselineOneBF~and \baselineTwoBF~based 
%?models do not gain any apparent advantage in conditioning on previous 
%?sentence selection decisions; this result suggests the need to improve
%?the representation of the summary as it is being constructed iteratively.
%?
%?\textbf{Choice of Encoder} We also find there to be no major advantage 
%?between the different sentence encoders. \textcolor{red}{In most cases,
%?there is no statistical significance between the averaging encoder and either
%?the RNN or CNN encoders.} 

%The lack of differentiation amongst the different encoders concerning; one
%would assume learning with the appropriate structure would be helpful.
%The results of next 




 \input{dl_based_salience_models/figures/table_fixed_vs_learned.tex}
\paragraph{Word Embedding Learning}
 Given that learning a sentence encoder (averaging has no learned parameters)
 does not yield significant improvement, it is natural to consider whether
 learning word embeddings is also necessary. 
 In \autoref{tab:embeddings} (Word Embedding Learning) we compare the 
 performance of different extractors
 using the averaging encoder, when the word embeddings are held fixed or 
 learned during training. In both cases, word embeddings are initialized with
 \textsc{Glove} embeddings trained on a combination of Gigaword and Wikipedia.
% \hal{TRAINED ON WHAT? JUST THE DEFAULT ONES?}
 When learning embeddings, words occurring 
 fewer than three times in the training data are mapped to an unknown
 token (with learned embedding).
 
% shows ROUGE recall
%when using fixed or updated word embeddings. 
 In all but one case,
fixed embeddings are as good or better than the learned embeddings.
This is a somewhat surprising finding on the CNN/DM data since it is reasonably
large, and learning embeddings should give the models more
flexibility to identify important word features.\footnote{The AMI corpus is an exception here where learning \emph{does} lead to small
performance boosts, however, only in the Seq2Seq extractor is this difference 
significant; it is quite possible that this is an artifact of the very small
test set size.}
%\hal{why is it surprising?} \textcolor{green}{[[CK: Because learning embeddings should give the model more capacity to represent important patterns]]}
This suggests that we cannot extract much generalizable learning signal 
from the content other than what is already present from initialization. 
Even on PubMed, where the language is quite different from the news/Wikipedia
articles the \textsc{Glove} embeddings were trained on, learning leads to 
significantly worse results.

%The language of this corpus is quite different from the 
%data that the GloVe embeddings were trained on and so it makes sense 
%that  there would be more benefit to learning word representations; one
%explanation for only seeing modest improvements is purely the small size
%of the test dataset which has only 20 training meetings.

%textcolor{red}{(NOTE TO CK -- expect learning to help on pubmed)}. \hal{yes, the dataset size is certainly an issue here. probably worth pointing this out. also when you learned the embeddings, did you initialize to pretrained embeddings? did you regularize toward them?}


 \input{dl_based_salience_models/figures/table_pos_ablations.tex}
\paragraph{POS Tag Ablation}
It is also not well explored what word features are being used by the encoders.
To understand which classes of words were most important we ran an ablation
study, selectively removing nouns, verbs 
(including participles and auxiliaries), adjectives \& adverbs, and 
function words (adpositions, determiners, conjunctions).
%Additionally, we ran ablation experiments
%using part-of-speech (POS) tags. \hal{this needs to be justified. why is this experiment interesting?}
%All datasets were automatically tagged using
%the spaCy part-of-speech (POS)
%tagger\footnote{https://github.com/explosion/spaCy}.   
%\kathy{I'm still curious what would happen if you separately removed all conjunction tags and later remaining POS.}
%We experimented with selectively removing 
%\begin{itemize}
%    \item nouns (NOUN and PROPN tags), 
%    \item verbs (VERB, PART, and AUX tags), 
%    \item adjectives/adverbs (ADJ and ADV tags), 
%    \item numerical expressions (NUM and SYM tags), and 
%    \item miscellaneous words (ADP, CONJ, CCONJ, DET, INTJ, and SCONJ tags)
%\end{itemize}
%from each sentece. 
The embeddings of removed words were replaced with a zero vector,
preserving the order and position of the non-ablated words in the sentence.
Ablations were performed on training, validation, and test partitions,
using the RNN extractor with averaging encoder.
\autoref{tab:ablations} (POS Ablations) shows the results of the POS
tag ablation experiments. 
While removing any word class from the representation generally hurts 
performance (with statistical significance), on the news domains,
the absolute values of the differences are quite small 
(.18 on CNN/DM, .41 on NYT, .3 on DUC) suggesting that the model's predictions
are not overly dependent on any particular word types.
On the non-news datasets, the ablations have a larger effect 
(max differences are 1.89 on Reddit, 2.56 on AMI, and 1.3 on PubMed).
Removing nouns leads to the largest drop on AMI and PubMed.
Removing adjectives and adverbs leads to the largest drop on Reddit,
suggesting the intensifiers and descriptive words are useful for 
identifying important content in personal narratives.
Curiously, 
removing the function word POS class yields a significant improvement
on DUC 2002 and AMI.


%The newswire domain does not appear to be sensative
%to these ablations; this suggests that the models are still able to identify
%the lead section of the document with the remaining word classes \textcolor{red}{(Verify this with histogram analysis)}. 
%The Reddit domain, which is not lead biased, is significantly effected.
%Notably, removing adjectives and adverbs results in a 1.8 point drop 
%in ROUGE-2 recall. 


 \input{dl_based_salience_models/figures/table_inorder_vs_shuffled.tex}
\paragraph{Document Shuffling} Sentence position is a well known and 
powerful feature for news summarization \citep{hong2014improving}, owing 
to the intentional lead bias in the news article writing\footnote{\url{https://en.wikipedia.org/wiki/Inverted_pyramid_(journalism)}}; it also explains the difficulty in beating
the lead baseline for single-document summarization 
\citep{nenkova2005automatic,rau:1999}.
In examining the generated summaries, we found
most of the selected sentences in the news domain came from the lead paragraph
%\hal{i feel like there must be citations to dig up here from like the 90s about lead summarization in news... it's also an intentional bias: maybe the right thing is to cite a style guide from a newsppaer that says to write this way}
of the document. This is despite the fact that there is a long tail of 
sentence extractions from later in the document in the ground truth extract 
summaries (31\%, 28.3\%, and 11.4\% of DUC, CNN/DM, and NYT training extract labels come 
from the second half of the document). 
%\hal{can you be more specific? like give some stats? what \%age come from first quarter of doc and what \%age from last half or something}. 
Because this lead bias is so strong, it is questionable whether
the models are learning to identify important content or just find the start
of the document. We conduct a sentence order experiment where 
each document's sentences are randomly shuffled during training. We then
%KM - I think below should be shuffled. I changed.
%CK - models are trained on shuffled data but evaluated on in order models.
%evaluate each model performance on the unshuffled test data, comparing to 
evaluate each model performance on the unshuffled test data, comparing to 
the model trained on unshuffled data; if the models trained on shuffled data
drop in performance, then this indicates the lead bias is the relevant factor.
%in learning content selection.

\autoref{tab:shuffle} (Document Shuffling) shows the results
of the shuffling experiments. 
The news domains and PubMed suffer a significant drop in performance 
when the document order is shuffled. By comparison, there is no significant difference between the shuffled and in-order models on 
the Reddit domain, and shuffling actually improves performance on AMI.
%\hal{what about in the cross-domain setting?} 
This suggest that position 
is being learned by the models in the news/journal article domain even when 
the model has no explicit position features, and that this feature is more 
important than either content or function words.












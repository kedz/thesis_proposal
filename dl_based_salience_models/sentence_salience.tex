

\subsection{Deep Learning Models of Sentence Salience}

 Given the diversity of neural architectural choices, a best practices
for sentence extracive summarization has yet to emerge. In this section
we ask what architecture design choices matter for single document 
summarization across a variety of domains.

We begin by definining some terminology. A sentence is represented as an
sequence of words $\sent = \word_1, \word_2, \ldots, \word_{|\sent|},$
where each word is drawn from a finite vocabulary $\vocab$ and $|\sent|$ is
the length of sentence $\sent$ in words. Similarly, a document $\doc =  
\sent_1, \sent_2, \ldots, \sent_{|\doc|} $ is a sequence of sentences, where 
$|\doc|$ is the size of the document in sentences.  

We treat sentence extractive summarization as a sequence tagging
problem: given a document $\doc$, we want to assign an associated binary
tag sequence $\Labels \in \{0,1\}^{\Sze{\doc}}$ such that the corresponding
set of extracts $\extracts = \{\sent_i \in \doc \; | \; \Labels_i =1 \}$ is
a suitable summary
of the document. Typically, it is assumed that the size of the extract summary
in sentences is much smaller than the input document, i.e. 
$|\extracts|  \ll |\doc|$.
It is also common to enforce a word budget $\budget$ such that
$\sum_{\sent \in \extracts} |\sent| \le \budget$.

A typical deep learning model will build up a hierarchical representation
of each sentence, starting at the word level, and then composing an arbitrarily
long sequence of word representations into a fixed length sentence 
representation.
First the individual words are 
projected to fixed length vectors, or word embeddings, via a mapping
$\embproj : \vocab
\rightarrow \mathbb{R}^{\embsize}$. The sentence encoder network $\encoder :
\{\mathbb{R}^{\embsize}\}^* \rightarrow \mathbb{R}^{\sembsize}$ is then 
responsible for mapping word embedding sequences to fixed length sentence 
embeddings. Finally, the sentence extractor network $\extractor : 
\{\mathbb{R}^\sembsize\}^* \rightarrow \{0, 1\}^*$ produces a label 
sequence $\Labels$.

We explore several choices of encoder and extractor architecture from the 
literature \citep{cheng2016neural,nallapati2016summarunner} as well as 
propose our own designs \citep{kedzie2018deep}. In the next sections,
we describe the different encoder and extractor architectures, before 
discussing data, experiments, and evaluation.

%the three sentence encoder architectures (\enAvg, 
%\enRnn, and \enCnn) followed by four extractor architectures 
%(\textit{rnn}, \textit{seq2seq}, \textit{sr}, and \textit{cl}).

\subsubsection{Sentence Encoders}
\paragraph{Averaging} This encoder simply averages a sentence's
associated word embeddings:
\[ \encoder_{avg}(\sent) = \frac{1}{|\sent|} \sum_{\word \in \sent} \embproj(\word). \]
Other than the word embeddings, this encoder involves no learned parameters,
and while it collapses word order, embedding averaging has consistently
been found competitive with more sophisticated sentence embedding techniques
\citep{iyyer2015deep,wieting2015towards,arora2016simple,wieting2017revisiting}.


\paragraph{Recurrent Neural Networks} The second encoder architecture
we experiment with is a recurrent neural network (RNN) over the word
embeddings. An RNN maintains an internal ``hidden'' state that is sequentially
updated upon observing each word embedding. In practice, we use a bidirectional
RNN with a gated recurrent unit (GRU) as the particular instantiation of 
the RNN cell \citep{cho2014learning}.
Under the RNN encoder, a sentence embedding for a sentence $\sent$ is defined 
as the concatenation of the final forward and backward GRU outputs,
\begin{align}
\encoder_{rnn}(\sent) = \Big[\overrightarrow{\semb}_{|\sent|}, \overleftarrow{\semb}_{1} \Big] & \\
  \rsemb_0 = \mathbf{0};& \quad 
  \rsemb_i = \rgru\left(\embproj(\word_i), \rsemb_{i-1}\right) \\
  \lsemb_{|\sent| + 1} = \mathbf{0};& \quad 
  \lsemb_i = \lgru\left(\embproj(\word_i), \lsemb_{i+1}\right) 
\end{align}
where $[\cdot]$ is the concatenation operator; $\rsemb_i$ and $\lsemb_i$ are
hidden states of the forward and backward GRU cells respectively; and 
$\rgru, \lgru : \mathbb{R}^\embsize \times \mathbb{R}^\sembsize 
\rightarrow \mathbb{R}^\sembsize$ are the forward and backward GRU cell 
operations.
\cite{nallapati2016summarunner} use a bidirectional RNN for their sentence
encoder.

\paragraph{Convolutional Neural Networks}
Our final sentence encoder uses a convolutional neural network
(CNN) to encode salient n-gram windows into a fixed length vector. 
CNN's have grown increasingly popular in many NLP tasks 
as a computationally efficient substitute for RNN-based architectures
\citep{kim2014convolutional,lei2015molding,dauphin2017language}.
Our architecture largely follows \cite{kim2014convolutional}: we apply a 
series of one dimensional convoluions over a sentence's word embeddings
using varying width convolutions. For each convolutional window size 
$\winsize \in \winsizes \subset \mathbb{N}$, a convolutional filter creates 
a feature vector $\cfeat{}{\winsize} \in \mathbb{R}^{\fmapsize_\winsize}$ 
and the encoder output is the concatenation of the $|\winsizes|$ vectors. 
The set of filter window sizes $\winsizes$ and the number of feature maps
$\fmapsize_\winsize$ for each $\winsize \in \winsizes$ are 
model hyperparameters.
Formally, we define the CNN encoding of a sentence $\sent$ as 
\begin{align}
\encoder_{cnn}(\sent) & = \left[\cfeat{}{\winsize} : \winsize \in \winsizes \right]\\
\cfeat{\cfidx}{\winsize} &= 
     \max_{i \in 1,\dots, |\sent| - \winsize + 1} 
       \relu\left(\cact_i \right) \\
\cact_i &= \cnnbias_\cfidx
    + \sum^{\winsize}_{j=1} \cnnweight_{\cfidx,j} \cdot \embproj(\word_{i + j -1})
\end{align}
where $\cnnbias \in \cnnBiasSpace$ and $\cnnweight \in \cnnWeightSpace$
are learned convolutional filter weights and $\relu(x) = \max(0, x)$ 
is the rectified linear unit \citep{nair2010rectified}. \cite{cheng2016neural}
use a CNN for their sentence encoder.

\subsubsection{Sentence Extractors}

A sentence extractor takes the encoder output, i.e. 
a sequence of sentence embeddings
$\encoder(\doc) =$ $\encoder(\sent_1), \ldots, \encoder(\sent_{|\doc|}) =
\semb_1, \ldots, \semb_{|\doc|}$, and produces an extract label sequence
$\Labels$. 
The sentence extractor is essentially a discriminative
classifier $p(\Labels_{1:\Sze{\doc}} | \semb_{1:\Sze{\doc}})$.
Previous neural network approaches to sentence extraction have assumed
an auto-regressive model, leading to a semi-Markovian
factorization of the extractor probabilities
$p(\Labels_{1:\Sze{\doc}} | \semb_{1:\Sze{\doc}})=\prod_{i=1}^{|\doc|} 
p(\Labels_i|\Labels_{<i},\semb_{1:\Sze{\doc}})$,
where each prediction $\Labels_i$ is dependent on \emph{all}
previous $\Labels_j$ for
all $j < i$. We compare two such models proposed by \cite{cheng2016neural}
and \cite{nallapati2016summarunner}.
A simpler approach that does not allow interaction among the $\Labels_{1:n}$
is to
model $p(\Labels_{1:\Sze{\doc}}|\semb_{1:\Sze{\doc}}) = 
\prod_{i=1}^{\Sze{\doc}} p(\Labels_i|\semb_{1:\Sze{\doc}})$,
  which we explore in two proposed extractor models that we refer to as the RNN 
  and Seq2Seq extractors.

\paragraph{SummaRunner Extractor}\citet{nallapati2016summarunner} proposed
a sentence extractor, which we refer to as the SummaRunner Extractor,
that factorizes the extraction probability into contributions 
from different sources.
First, a bidirectional RNN is run over the sentence embeddings\footnote{\citet{nallapati2016summarunner}
    use an RNN sentence encoder with 
this extractor architecture; in this work we pair the SummaRunner extractor
with different encoders. } and the output is
concatenated. A representation of the whole document is made by 
averaging the RNN output. A summary representation is also constructed 
by taking the sum of the previous RNN outputs weighted by their extraction
probabilities. Extraction predictions are made using 
the RNN output at the $i$-th prediction step, the document representation, and 
$i$-th version of the summary representation, along with factors for 
sentence location in the document. The use of the iteratively constructed
summary representation creates a dependence of $\Labels_i$ on all 
$\Labels_{<i}$.


In more detail, first the bidirectional RNN is applied to the input 
sentence embeddings,
\begin{align}
    \rExtHid_0 = \textbf{0}&\quad \quad \rExtHid_i = \rgru(\semb_i, \rExtHid_{i-1}) \\
    \lExtHid_{\Sze{\doc} + 1} = \textbf{0}& \quad \quad \lExtHid_i = \lgru(\semb_i, \lExtHid_{i+1}).
\end{align}

Then a document embedding $\demb$ is created by averaging in the hidden
RNN states and passing them through a feedforward layer: 
\begin{align}
    \demb = \tanh\left(\mathbf{b}_\doc + \mathbf{W}_\doc\frac{1}{\Sze{\doc}}
    \sum_{i=1}^{\Sze{\doc}} \left[\rExtHid_i; \lExtHid_i\right] \right).
\end{align}

The RNN outputs are then passed through a separate fully connected layer to 
create a sentece representation $\extHid_i$ where 
\begin{align}
    \extHid_i &= \relu\left(\mathbf{b}_z + \mathbf{W}_z \left[ \rExtHid,\lExtHid\right]\right)
\end{align}

The extraction probability is then determined by contributions from five 
sources:
\begin{align}
    \textit{sentence salience} &\quad a^{(sent)}_i=\mathbf{W}^{(sent)} \extHid_i, \\
    \textit{document salience}&\quad a^{(doc)}_i = \extHid_i^T\mathbf{W}^{(doc)} \demb, \\
    \textit{summary novelty}&\quad a^{(nov)}_i = -\extHid_i^T\mathbf{W}^{(nov)} \smemb_i, \label{eq:srnov} \\
    \textit{fine-grained position}&\quad a^{(fpos)}_i = \mathbf{W}^{(fpos)} 
    \posemb_i, \\
    \textit{coarse-grained position}&\quad a^{(cpos)}_i = \mathbf{W}^{(cpos)} 
    \qrtemb_i,
\end{align}
where $\posemb_i$ and $\qrtemb_i$ are embeddings associated with the $i$-th sentence
position and the quarter of the document containing sentence $i$ respectively.
In \autoref{eq:srnov}, $\smemb_i$ is a representation of the summary after
making the first $i-1$ predictions, and is 
computed as the
sum of the previous $\extHid_{<i}$ weighted by their extraction probabilities,
\begin{align}
    \smemb_i & = \tanh\left(\sum_{j=1}^{i-1} p(\Labels_j=1|\Labels_{<j},\semb_{1:\Sze{\doc}}) \cdot \extHid_j\right), \quad\quad \smemb_1 = \mathbf{0}.
\end{align}
Note that the presence of this term induces dependence of each 
$\Labels_i$ to 
all $\Labels_{<i}$. % similarly to the Cheng \& Lapata extractor.
The final extraction probability is the logistic sigmoid of the
sum of these terms plus a bias,
\begin{align}
    p(\Labels_i=1|\Labels_{<i}, \semb_{1:\Sze{\doc}}) &= \sigma\left(
      a_i^{(sent)} + a_i^{(doc)} + a_i^{(nov)} 
  + a_i^{(fpos)}  + a_i^{(cpos)} + b \right).
    %p(\Labels_i=1|\Labels_{<i}, \semb_{1:\Sze{\doc}}) &= \sigma\left(\begin{array}{l}
 %     a_i^{(con)} + a_i^{(sal)} + a_i^{(nov)} \\
 % + a_i^{(pos)}  + a_i^{(qrt)} + b \end{array}\right).
\end{align}
The weight matrices $\mathbf{W}_\doc$, $\mathbf{W}_z$, $\mathbf{W}^{(sent)}$,
$\mathbf{W}^{(sal)}$, $\mathbf{W}^{(nov)}$, $\mathbf{W}^{(fpos)}$,
$\mathbf{W}^{(cpos)}$ and bias terms $\mathbf{b}_\doc$, $\mathbf{b}_z$, and 
$b$ 
are learned parameters;
additionally, the GRUs have separate learned parameters.
%

\paragraph{RNN Extractor \citep{kedzie2018deep}}
    Our first proposed model is a very simple bidirectional
RNN based tagging model, and can be thought of as a 
bare bones version of the SummaRunner
extractor. In this model, we pass the outputs of a bidirectional GRU over the 
sentence embeddings into a multi-layer perceptron that terminates in a layer 
of
sigmoid activations corresponding to the sentence level extraction probabilities.
%sentence encoder we use a GRU cell.
%The forward and backward outputs of each sentence are passed through a 
%multi-layer perceptron with a logsitic sigmoid output 
%to predict the probability
%of extracting each sentence. 
%See \autoref{fig:extractors}.a for a graphical layout.
%and \autoref{app:rnnextractor} for details.


%\newcommand{\rExtHidden}{\overrightarrow{h}}
%\newcommand{\lExtHidden}{\overrightarrow{h}}
%\newcommand{\docSize}{|\doc|}
%\newcommand{\logits}{o}

\begin{align}
    \rExtHid_0 = \mathbf{0} &\quad\quad   \rExtHid_i = \rgru(\semb_i, \rExtHid_{i-1}) \\
    \lExtHid_{\Sze{\doc} + 1} = \mathbf{0}&\quad \quad    \lExtHid_i = \lgru(\semb_i, \lExtHid_{i+1}) \\
    \mathbf{o}_i &= \relu\left(\mathbf{U} \cdot [\rExtHid_i; \lExtHid_i] + \mathbf{u} \right)\\
    p(\Labels_i=1|\semb_{1:\Sze{\doc}}) &= \sigma\left(\mathbf{v}\cdot \mathbf{o}_i + v  \right)
\end{align}
where $\rgru$ and $\lgru$ indicate the 
forward and backward GRUs respectively, and each have separate learned 
parameters; $\mathbf{U}, \mathbf{v}$ and $u, v$ are learned weight and 
bias parameters respectively.


\paragraph{Cheng \& Lapata Extractor} 
 This extractor, proposed by 
\citet{cheng2016neural}, %, which we refer to as the Cheng \& Lapata Extractor,
is built around a sequence-to-sequence model.
First, each sentence embedding\footnote{\citet{cheng2016neural} used an CNN sentence encoder with 
this extractor architecture; in this work we pair the Cheng \& Lapata extractor
with several different encoders.} is
fed into an encoder side RNN, with the final encoder state passed to the
first step of the decoder RNN. On the decoder side, the same sentence 
embeddings are fed as input to the decoder and both encoder and decoder 
outputs are used to predict each $\Labels_i$. The decoder input is weighted by the previous 
extraction
probability, inducing the dependence of $\Labels_i$ on $\Labels_{<i}$.


The basic architecture is a
sequence-to-sequence
model defined as follows:
\begin{align}
    \enextHid_0 = \textbf{0}& \quad \quad   \enextHid_i = \gru_{enc}(\semb_i, \enextHid_{i-1}) \\
    \deextHid_1 = \gru_{dec}(\semb_*, \enextHid_{\Sze{\doc}}) &\quad\quad
\deextHid_i = \gru_{dec}(p_{i-1} \cdot \semb_{i-1}, \deextHid_{i-1}) \label{eq:cl1} \\
\mathbf{o}_i &= \relu\left(\mathbf{U} \cdot [\enextHid_i; \deextHid_i] + \mathbf{u} \right)\\
p_i = p(\Labels_i=1|\Labels_{<i}, \semb_{1:\Sze{\doc}}) &= \sigma\left(\mathbf{v}\cdot \mathbf{o}_i + v  \right) 
\end{align}
where $\semb_*$ is a learned ``begin decoding'' sentence embedding;
each GRU has separate learned 
parameters; and $\mathbf{U}, \mathbf{v}$ and $u, v$ are learned weight and bias parameters.
Note in \autoref{eq:cl1} that 
the decoder side GRU input is the sentence embedding from the previous time
step weighted by its probabilitiy of extraction ($p_{i-1}$) from the 
previous step, inducing dependence of each output $\Labels_i$ on all previous 
outputs $\Labels_{<i}$.



\paragraph{Seq2Seq Extractor \citep{kedzie2018deep}} 
%One shortcoming of the RNN extractor is that long range
%information from one end of the document may not easily be able to affect 
%extraction probabilities of sentences at the other end. 
Our second proposed model, the represents a more typical sequence-to-sequence
architecture with dot product style attention \citep{luong2015effective},
%mitigates this problem with an 
%attention 
%mechanism commonly
used for neural machine translation \cite{bahdanau2014neural} and 
abstractive summarization \cite{see2017get}. 
The sentence embeddings are first
encoded by a bidirectional $\gru$. A separate decoder $\gru$ transforms each 
sentence into a query vector which attends to the encoder output. The
attention weighted encoder output and the decoder $\gru$ output are concatenated
and fed into a multi-layer perceptron to compute the extraction probability.
%See \autoref{fig:extractors}.b for a graphical layout.
Unlike the Cheng \& Lapata extractor, the Seq2Seq extractor does not induce 
dependencies between the extraction labels, i.e. they are conditionally independent.

The architecture is defined as follows
\begin{align}
    \enextHid_0 = \textbf{0}& \quad \quad   \enextHid_i = \gru_{enc}(\semb_i, \enextHid_{i-1}) \\
    \deextHid_0 = \gru_{dec}(\semb_*, \enextHid_{\Sze{\doc}}) &\quad\quad
\deextHid_i = \gru_{dec}(\semb_{i}, \deextHid_{i-1}) \label{eq:cl1} \\
 \alpha_{i,j} = 
   \frac{\exp \left(\deextHid_i \cdot \enextHid_j \right)}{
   \sum_{j=1}^{\Sze{\doc}}\exp\left(\deextHid_i \cdot \enextHid_j\right)}
& \quad\quad \bar{\enextHid}_i = \sum_{j=1}^{\Sze{\doc}} \alpha_{i,j} \enextHid_j \\
\mathbf{o}_i &= \relu\left(\mathbf{U} \cdot [\bar{\enextHid}_i; \deextHid_i] + \mathbf{u} \right)\\
p(\Labels_i=1|\semb_{1:\Sze{\doc}}) &= \sigma\left(\mathbf{v}\cdot \mathbf{o}_i + v  \right) 
\end{align}
where $\semb_*$ is a learned ``begin decoding'' sentence embedding;
each GRU has separate learned 
parameters; and $\mathbf{U}, \mathbf{v}$ and $u, v$ are learned weight and 
bias parameters. In practice, we run this model in ``bidirectional mode'' 
where seperate sequence-to-sequence model also run in the reverse direction,
equations 28-29 are computed using the concatenation of the encoder/decoder
outputs. 

\subsubsection{Data}
\input{dl_based_salience_models/figures/datasets.tex}
We perform our experiments across six corpora from varying domains to 
understand how different biases within each domain can affect content 
selection. The corpora come from the news domain
(CNN-DailyMail, New York Times, DUC), personal narratives domain (Reddit),
workplace meetings (AMI), and medical journal articles (PubMed). See 
\autoref{tab:data} for dataset statistics.


\paragraph{CNN-DailyMail} We use the preprocessing and training, validation, 
and test splits
of \cite{see2017get}.
This corpus is a mix of news on different topics including politics,
sports, and entertainment.

\paragraph{New York Times}The New York Times (NYT) corpus \citep{sandhaus2008new} contains
 two types of abstracts for a subset of its articles. The first summary is
an archival abstract and the 
second is a shorter online teaser meant to entice a viewer of the webpage to
click to read more. From this collection, we take all articles that have 
a concatenated summary length of at least 100 words.
We create training, validation, and test splits by partitioning on dates;
we use the year 2005 as the validation data, with training and test partitions
including documents before and after 2005 respectively.

\paragraph{DUC} We use the single document summarization data from the 2001
and 2002
Document Understanding Conferences (DUC) \citep{over2002introduction}. We split the 2001 data into training
and validation splits and reserve the 2002 data for testing.

\paragraph{AMI} The AMI corpus \citep{carletta2005ami} 
is a collection of real and staged office meetings
annotated with text transcriptions, along with abstractive
summaries. We use the prescribed splits. 

\paragraph{Reddit} \citet{ouyang2017crowd} collected a corpus of personal 
    stories shared
 on Reddit\footnote{\url{www.reddit.com}} along with multiple extractive 
 and abstractive summaries. We randomly split this data using roughly three and five percent of the data validation and test respectively.

\paragraph{PubMed}{We created a corpus of 25,000 randomly sampled
    medical journal articles from the PubMed Open Access 
    Subset\footnote{\url{https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/}}.
    We only included articles if they were at least 1000 words long and 
    had an abstract of at least 50 words in length.
We used the article abstracts as the ground truth human summaries.}

\paragraph{Ground Truth Extract Summaries}
Since we do not typically have ground truth extract summaries from which to
create the labels $\Labels_i$, we construct gold label sequences 
by greedily optimizing \rougeN{1} as in \cite{nallapati2016summarunner}.
We choose to optimize for \rougeN{1} rather than 
\rougeN{2} similarly to other optimization based approaches to summarization 
\cite{sipos2012large,durrett2016learning,nallapati2016summarunner} 
which found this to be the easier target to learn.






\subsubsection{Experiments}

 We are interested in two questions. The first, more pragmatic question, is
 what are the best configuration of encoder/extractor architectures?
 We answer this question by evaluating \rouge{} recall and \meteor{} 
\citep{denkowski:lavie:meteor-wmt:2014}
 performance across our six collected datasets. We perform the standard
 stochastic gradient descent based optimization (using the Adam
 update \citep{kingma2014adam}) of the weighted negative log likehood 
 \[ \mathcal{L}(\theta) = -\sum_{\doc, \Labels \in \mathcal{D}} 
            \sum_i \omega(\Labels_i) 
        \log p(\Labels_i|\Labels_{<i}, \encoder(\doc); \theta) \]
        where $\theta$ are model parameters and $\omega(\Labels_i)$ upweights
        the positive labels to account for the imbalanced label distribution.

% We lack human reference extract labels for our datasets and so we obtain
% said lable sequences heuristically, by finding a label sequence $\Labels^*$
% by greedily optimizing ROUGE-1 recall with respect to the human reference
% abstracts.

 The second question, is more diagnostic in nature: what signals
 in the data are driving model learning?
 We perform several experiments to find answers. 
 We hypothesize that the lexical semantics encoded at the word embedding
 level will be important to subsequent sentence representations, and
 perform a comparison on learning with and with out fine tuning of the 
 embeddings. In both cases, embeddings are initialized with Glove
 embeddings pretrained on Wikipedia and Gigaword \citep{pennington2014glove}.
 
 
 We also hypothesize that certain classes of words will be more important 
 to identifying salient content than others. We perform word ablation 
 experiments where we alternately remove nouns, verbs, adjectives \& adverbs,
 and function words from the sentence encoder input and compare performance 
 to the non-ablated system. We expect that the nouns will be more important
 to content selection. 


 Our final experiments attempt to tease out the effect of structural features 
 from the lexical. In this experiment, we shuffle the sentence order at 
 training time. In this setup, we obfuscate features about which content 
 was introduced in the article first, an important and well known bias in 
 news domain \citep{nenkova2005automatic}. 

 

 \subsubsection{Results}

 \input{dl_based_salience_models/figures/table_overall_results.tex}
  

The results of our main experiment comparing 
the different extractors/encoders are shown in 
Table~\ref{tab:results}.
Overall, we find no major advantage when using the CNN and RNN sentence
encoders over the averaging encoder. The best performing encoder/extractor pair either 
uses the averaging 
encoder (five out of six datasets) or the differences 
are not statistically significant. %When only comparing within the 
%same extractor choice,  the averaging encoder is the better choice
%in 14 of 20 cases. 
%\hal{i wonder if it would be worth adding another ``average performance metric'' column to \autoref{tab:results}.
%  i'm thinking have ``Average $\Delta$-Best'' meaning how far (on average across the datasets) is this setting from the best setting available on that dataset.
%  so since the best numbers are: 25.56, 35.85, 23.11, 13.65, 5.63
%  and the first row numbers are: 25.42, 34.67, 22.65, 11.37, 5.50
%  then the deltas are:            0.14,  1.18,  0.46,  2.28, 0.13
%  the the average delta is 0.84 (assuming my math is right)
%  there's an argument to do multiplicative, in which case
%  the multipliers for first row:  0.99,  0.97,  0.98,  0.83, 0.97
%  and the average is 0.95
%  either way this gives a quick way to make comparisons between rows. you could do the same for the other tables too.}

  %\hal{in some of the tables you list R-2 as headers even though all the numbers are R-2. just put that in the caption.}


When looking at extractors, the Seq2Seq extractor is either part of 
the best performing system (three out of six datasets) or is not 
statistically distinguishable from the best extractor. 

Overall, on the news and medical journal domains, the differences are 
quite small with the 
differences between worst and best systems on the CNN/DM dataset 
spanning only .56 of a ROUGE point. While there is more performance variability
 in the Reddit and AMI data, there is less distinction among systems: 
 no differences are significant on Reddit
and every extractor has at least one configuration that is indistinguishable
from the best system on the AMI corpus. This is probably due to the small test
size of these datasets.
%\hal{this is probably at least partially because of test set size. maybe mention this.}





%?\textcolor{red}{Overall we find that the \modelTwoBF~extractor achieves the 
%?best ROUGE scores on three out of four domains (STILL RUNNING ON AMI AND PUBMED). 
%?However, most
%?differences are not signficant. (Need to discuss stat sig and how to show it).}
%?On the larger CNN-DailyMail dataset, especially, 
%?differences are quite smail across all extractor/encoder pairs.
%?The \baselineOneBF~extractor achieves the best performance on the DUC 2002
%?dataset. It is disappointing that the \baselineOneBF~and \baselineTwoBF~based 
%?models do not gain any apparent advantage in conditioning on previous 
%?sentence selection decisions; this result suggests the need to improve
%?the representation of the summary as it is being constructed iteratively.
%?
%?\textbf{Choice of Encoder} We also find there to be no major advantage 
%?between the different sentence encoders. \textcolor{red}{In most cases,
%?there is no statistical significance between the averaging encoder and either
%?the RNN or CNN encoders.} 

%The lack of differentiation amongst the different encoders concerning; one
%would assume learning with the appropriate structure would be helpful.
%The results of next 




 \input{dl_based_salience_models/figures/table_fixed_vs_learned.tex}
\paragraph{Word Embedding Learning}
 Given that learning a sentence encoder (averaging has no learned parameters)
 does not yield significant improvement, it is natural to consider whether
 learning word embeddings is also necessary. 
 In \autoref{tab:embeddings} we compare the performance of different extractors
 using the averaging encoder, when the word embeddings are held fixed or 
 learned during training. In both cases, word embeddings are initialized with
 GloVe embeddings trained on a combination of Gigaword and Wikipedia.
% \hal{TRAINED ON WHAT? JUST THE DEFAULT ONES?}
 When learning embeddings, words occurring 
 fewer than three times in the training data are mapped to an unknown
 token (with learned embedding).
 
% shows ROUGE recall
%when using fixed or updated word embeddings. 
 In all but one case,
fixed embeddings are as good or better than the learned embeddings.
This is a somewhat surprising finding on the CNN/DM data since it is reasonably
large, and learning embeddings should give the models more
flexibility to identify important word features.\footnote{The AMI corpus is an exception here where learning \emph{does} lead to small
performance boosts, however, only in the Seq2Seq extractor is this diference 
significant; it is quite possible that this is an artifact of the very small
test set size.}
%\hal{why is it surprising?} \textcolor{green}{[[CK: Because learning embeddings should give the model more capacity to represent important patterns]]}
This suggests that we cannot extract much generalizable learning signal 
from the content other than what is already present from initialization. 
Even on PubMed, where the language is quite different from the news/Wikipedia
articles the GloVe embeddings were trained on, learning leads to 
significantly worse results.

%The language of this corpus is quite different from the 
%data that the GloVe embeddings were trained on and so it makes sense 
%that  there would be more benefit to learning word representations; one
%explanation for only seeing modest improvements is purely the small size
%of the test dataset which has only 20 training meetings.

%textcolor{red}{(NOTE TO CK -- expect learning to help on pubmed)}. \hal{yes, the dataset size is certainly an issue here. probably worth pointing this out. also when you learned the embeddings, did you initialize to pretrained embeddings? did you regularize toward them?}


 \input{dl_based_salience_models/figures/table_pos_ablations.tex}
\paragraph{POS Tag Ablation}
It is also not well explored what word features are being used by the encoders.
To understand which classes of words were most important we ran an ablation
study, selectively removing nouns, verbs 
(including participles and auxiliaries), adjectives \& adverbs, and 
function words (adpositions, determiners, conjunctions).
%Additionally, we ran ablation experiments
%using part-of-speech (POS) tags. \hal{this needs to be justified. why is this experiment interesting?}
All datasets were automatically tagged using
the spaCy part-of-speech (POS)
tagger\footnote{https://github.com/explosion/spaCy}.   
%\kathy{I'm still curious what would happen if you separately removed all conjunction tags and later remaining POS.}
%We experimented with selectively removing 
%\begin{itemize}
%    \item nouns (NOUN and PROPN tags), 
%    \item verbs (VERB, PART, and AUX tags), 
%    \item adjectives/adverbs (ADJ and ADV tags), 
%    \item numerical expressions (NUM and SYM tags), and 
%    \item miscellaneous words (ADP, CONJ, CCONJ, DET, INTJ, and SCONJ tags)
%\end{itemize}
%from each sentece. 
The embeddings of removed words were replaced with a zero vector,
preserving the order and position of the non-ablated words in the sentence.
Ablations were performed on training, validation, and test partitions,
using the RNN extractor with averaging encoder.
\autoref{tab:ablations} shows the results of the POS
tag ablation experiments. 
While removing any word class from the representation generally hurts 
performance (with statistical significance), on the news domains,
the absolute values of the differences are quite small 
(.18 on CNN/DM, .41 on NYT, .3 on DUC) suggesting that the model's predictions
are not overly dependent on any particular word types.
On the non-news datasets, the ablations have a larger effect 
(max differences are 1.89 on Reddit, 2.56 on AMI, and 1.3 on PubMed).
Removing nouns leads to the largest drop on AMI and PubMed.
Removing adjectives and adverbs leads to the largest drop on Reddit,
suggesting the intensifiers and descriptive words are useful for 
identifying important content in personal narratives.
Curiously, 
removing the function word POS class yields a significant improvement
on DUC 2002 and AMI.


%The newswire domain does not appear to be sensative
%to these ablations; this suggests that the models are still able to identify
%the lead section of the document with the remaining word classes \textcolor{red}{(Verify this with histogram analysis)}. 
%The Reddit domain, which is not lead biased, is significantly effected.
%Notably, removing adjectives and adverbs results in a 1.8 point drop 
%in ROUGE-2 recall. 


 \input{dl_based_salience_models/figures/table_inorder_vs_shuffled.tex}
\paragraph{Document Shuffling} Sentence position is a well known and 
powerful feature for news summarization \cite{hong2014improving}, owing 
to the intentional lead bias in the news article writing\footnote{\url{https://en.wikipedia.org/wiki/Inverted_pyramid_(journalism)}}; it also explains the difficulty in beating
the lead baseline for single-document summarization 
\cite{nenkova2005automatic,rau:1999}.
In examining the generated summaries, we found
most of the selected sentences in the news domain came from the lead paragraph
%\hal{i feel like there must be citations to dig up here from like the 90s about lead summarization in news... it's also an intentional bias: maybe the right thing is to cite a style guide from a newsppaer that says to write this way}
of the document. This is despite the fact that there is a long tail of 
sentence extractions from later in the document in the ground truth extract 
summaries (31\%, 28.3\%, and 11.4\% of DUC, CNN/DM, and NYT training extract labels come 
from the second half of the document). 
%\hal{can you be more specific? like give some stats? what \%age come from first quarter of doc and what \%age from last half or something}. 
Because this lead bias is so strong, it is questionable whether
the models are learning to identify important content or just find the start
of the document. We conduct a sentence order experiment where 
each document's sentences are randomly shuffled during training. We then
%KM - I think below should be shuffled. I changed.
%CK - models are trained on shuffled data but evaluated on in order models.
%evaluate each model performance on the unshuffled test data, comparing to 
evaluate each model performance on the unshuffled test data, comparing to 
the model trained on unshuffled data; if the models trained on shuffled data
drop in performance, then this indicates the lead bias is the relevant factor.
%in learning content selection.

\autoref{tab:shuffle} shows the results
of the shuffling experiments. 
The news domains and PubMed suffer a significant drop in performance 
when the document order is shuffled. By comparison, there is no significant difference between the shuffled and in-order models on 
the Reddit domain, and shuffling actually improves performance on AMI.
%\hal{what about in the cross-domain setting?} 
This suggest that position 
is being learned by the models in the news/journal article domain even when 
the model has no explicit position features, and that this feature is more 
important than either content or function words.












\section{Related Work}


The complete range of methods and task variations covered by the summarization
literature are too numerous to describe completely here. In the space 
available, we give an overview of unsupervised and supervised learning
approaches to estimating word and sentence importance. 
We also give some background on the variety
of summarization tasks, and the manner in which they are evaluated.


\subsection{Unsupervised Word Importance}


The most obvious signal available for estimating word importance is word 
frequency, i.e. frequent word occurences (modulo stopwords) are likely 
to be related the important topics of the document or document collection 
\cite{topic_sigs}, and it is the chief ingredient in all of the following
word importance estimators.
Term weighting schemes from information retrieval
have frequently been used, most commonly tf-idf 
\cite{centroid,maybe_conroy,find_others}. 
Various probabilistic weightings have also been considered, e.g. the
observed document probability \cite{freq_sum} or the ratio of background
corpus probabiltiy to document probability \cite{topic_sigs}.
Information theoretic transformations of word frequency like the document
level term entropy
have also been found to be useful \cite{klsum,reference_less_summarization}.
Latent semantic analyis (LSA), latent dirichlet allocation (LDA), and
other matrix factorization schemes have been used to find a low dimensional
``concept'' representation for each word, the sum of the concept contributions
constituting the word importance score \cite{multiling_stuff}.
Finally, various methods of transforming word co-occurences into a word graph
have been used in conjunction with unsupervised graph ranking algorithms
\cite{lexrank,conroy_multiling,zhao_2009}.


\subsection{Unsupervised Sentence Importance}

 Graph based ranking that has been used for word importance has similarly 
been used to estimate sentence importance, where the graph consists of 
sentence vertices and edge weights are computed using pairwise sentence 
similarity \cite{textrank}. Another popular method is the centroid method
\cite{centroid} where an average vector representation for a document set is 
constructed and sentences are ranked by their similarity to the centroid object.
Sentence clustering methods have also been applied. In all these methods, 
sentences are tpyically represented as term vectors where the weights for 
each term item are determined by one of the methods mentioned in word importance
estimation methods. 


\subsection{Supervised Word Importance}

The growing availibilty of large text collections with summaries has also
encouraged the development of many supervised methods for learning the word 
importance 
directly from the data. \cite{Regsum} use many of the unsuperivsed word  
importance measures, (e.g. frequency, entropy, document position) to estimate
the probabiltiy of a word occuring in a human abstract. Learning-to-rank
has also been explored in a variety of contexts. For example in single
document summarization, \cite{lapata} used convolutional neural networks
learn rankings and while \cite{guo,macreadie} use a feature based model
to rank sentences in a streaming summarization task.
Word importance weights have also been learned in combinatorial optimization
based approaches to summarization; in several integer linear program (ILP)
formulations of the summarization problem unigram and bigram weights 
are learned \cite{gillick,martens,berkely}, in addition to learning based
submodular optimization approaches \cite{submod,svm}.




\subsection{Supervised Sentence Methods}


Structured prediction methods have also been explored. The most common is
to model the sentence selection problem as an integer linear program (ILP)
and learning typically happens in a large margin/structured svm framework 
\cite{durret}. In addition to learning ngram feature weights, discourse,
and redundancy features can also be modeled \cite{somebody}. 
Deletion based compression 
and phrase substitution have also been modeled using ILPs, where non-essential
phrases can be discarded or shortened, and entity references rewritten, e.g.
turning a pronominal reference into a nominal one.

Sentence selection can be viewed as a submodular set optimization 
(submodularity in set theory is analagous to convexity in functional analysis)
where greedy selection algorithms can efficiently find the best subset of 
sentences according to a set scoring function and budget constraints.
Learning of the scoring function has been done by \cite{uwsipos} using 
linear functions on ngram features. Feature weights can similarly be learned 
in a large margin svm framework.


Sentence selection can be modeled as a sequence tagging problem, e.g. using
hidden Markov models \cite{conroy}.

Dependencies between sentence selection can be difficult, and so in the
spirit of naive Bayes, it can be fruitful to model sentence selection as 
independent classification problems. For example, \cite{kupiec} use ....


\cite{drago_nn} use a graph convolutional neural network on top of a sentence
graph constructed from either discourse structure or sentence similarity.


Learning to rank methods have been explored, see \cite{lapata}.

\subsection{Text Generation}


While there are a variety of text generation methods, we cover three main
approaches, template based generation, sentence fusion, and 
sequence-to-sequence models.



\paragraph{Template-Based Generation} 


\paragraph{Sentence Fusion} Sentence fusion attempts to combine two or
more sentences into one. This is typically done by selecting one sentence
as the backbone, and then aligning the remaing sentences to the backbone.
Heuristics are used for backbone selection and alignment. This method
is better suited to cases where there is much redundancy between candidate
summary sentences since fusing radically different sentences is likely to
yield misleading results.
this approach is that it is difficult 



Thanks to recent advances in neural machine translation (NMT), we now
have flexible, high capacity models for generating free form text from
an arbitrary sequence input. This has led to a host of work exploring 
variety for attention based models to the summarization task \cite{everybody}.
Adaptations to attention like copying have been found to be helpful for 
handling out of vocabulary terms and controlling for repetitive outputs 
\cite{seeandothers}.
One difficulty with this approach is that is hard to control the output,
spurring additional research in more controllable neural text generation.
Length \cite{lc1,lc2}, sentiment \cite{contr}, and domain \cite{socher_I_think}
have been explored, typically by treating the feature under control 
as a learned embedding that is fed as an additional input into the generator.
\cite{lapata} explore a method similar too our faithful text generation model,
however we intend to explore some differences.




 


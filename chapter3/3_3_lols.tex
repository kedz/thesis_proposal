\subsection{Model 2: LOLS}

Our next model attempts to address some of the shortcomings of the 
first. First the previous model process sentences in hourly batches.
Ideally we would process new documents as soon as possible in order 
to minimize the negative effects of latency. Second, our salience
regressors were statically trained and did not take advantage of features
like the similarity to recently selected updates and relied mostly on
tuned thresholds to account for redundancy. 

Getting around these limitations poses some severe challenges. Using
dynamic features means that naive traing will require gold reference
sentence extracts and that naively training a sentence salience model
with these will under explore the space of plausable update summaries,
meaning that in practice errors are likely to snowball over time.

Instead, we develop a clairvoyant oracle summarizer $\pi^o$ whose behavior
we want to imitate. $\pi^o$ has knowledge of what nuggets, if any, 
are present in each sentence in the document stream, and it's behavior
is quite simple: it processes each sentence in a query's relevant document 
stream and 
when it encounters a sentence with a novel nugget, it adds that sentence
to the update summary.

As we stated before, training only on a single oracle run over document stream
would be sub-optimal because the oracle is perfect and it would finish 
recovering all of the nuggets quite quickly and then do nothing for
the remainder of the stream. In practice, our learned model is likely to
make mistakes, missing the first few appearances of a nugget but hopefully
recovering them as repetition in the stream makes them more likely to be 
selected. Only following the oracle's first best pass would not help us learn
to recover from errors.

To make better use of the oracle, we adopt the locally optimal learning to
search (LOLS) algorithm \cite{lols}, one of a family of learning to search 
(L2S) algorithms. In order to formally describe the algorithm, we first 
introduce some notation. We treat the streaming summarization problem
as a markov decision process. At each timestep $t$ we observe a state
$s_t \in \mathcal{S}$ and $t$-th sentence $x_t \in \mathcal{X}$ from the 
document stream.
A policy $\pi : \mathcal{S} \times \mathcal{X} \rightarrow \{0, 1\}$
maps a state-sentence tuple to an action $a \in \mathcal{A} =\{0,1\}$
where $a=1$ indicates we extract the sentence and $a=0$ indicates we skip
the current sentence. The transition function 
$d : \mathcal{S} \times \mathcal{X} \times \mathcal{A} \rightarrow \mathcal{S}$
deterministically maps state-sentence-action tuples to the next state. 
In practice the state contains set of sentences previously extracted and
other rolling statistics from previously observed sentences in the stream.
Our training objective is to minimize the expected costs of each action:
\[ \mathcal{L}(\pi) = \mathbb{E}_{s,x \sim \pi} \left[ c\left(\pi(s,x), s, x\right) \right] \]
where $c :\mathcal{A} \times \mathcal{S} \times \mathcal{X} \rightarrow \mathcal{A}$ is the cost of taking a given action in a particular state-sentence
observation.

~\\

~\\

In the LOLS training regime, there are two phases: the 
\emph{roll-in} and the \emph{roll-out}. The roll-in phase 

at each timestep 
we alternate between using the oracle $pi^o$ or our current learned policy
$\hat{\pi}$ to run multiple time steps into the future and get a cost for 
a hypothetical summary we would have created in the case where we extracted the 
current sentence or skipped it. We collect these scores for each decision
and update a regression model to accuractely predict the cost of each action
given the current sentence and update summary/stream state.





\subsubsection{Data}
We ran 

\subsubsection{Oracle Policy and Loss Function}

We use a greedy oracle that selects sentences that contain novel
nuggets. This oracle will achieve an optimal Comprehensiveness score, i.e.
it will obtain every possible novel nugget in the roll-out phase.
However, it will not always achieve the maximum possible Expected Gain.
For example, consider the sequence of sentences $s_1, s_2, s_3$, where
nugget $n_1 \in s_1$, $n_2 \in s_2$, and $n_1,n_2 \in s_3$. The greedy oracle,
proceeding sequentially would select sentences $s_1$ and $s_2$, and skip
$s_3$, achieving an Expected Gain of $\frac{|\{n_1, n_2\}|}{|\{s_1, s_2 \}|} = \frac{2}{2} = 1$. The maximum achievable Expected Gain is obtained by skipping
the first two sentences and selecting sentence $s_3$ yielding $\frac{|\{n_1, n_2\}|}{|\{s_3 \}|} = \frac{2}{1} = 2$. In practice we are far from matching
the greedy oracle and so it suffices for now as an aspirational target.



We used the complement of Dice coefficient as the loss function.



\subsubsection{Experiments }

In our results, we refer to our learn-to-search approach as LS.
We compare to a lead sentence baseline that takes the first sentence
of every document as an update if it's maximum cosine similarity to any 
previous update was below a threshold. We refer to this method as COS.

We also compare our 








suboptimal in that it will achieve a perfect
Comprehensiveness 



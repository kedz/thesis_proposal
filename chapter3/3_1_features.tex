\input{chapter3/figures/3_fig_features.tex}

\subsection{Features for Sentence Importance Estimation}

The streaming summarization problem is difficult precisely because the context
is constantly shifting. We cannot rely solely on word frequency because
the counts of particular ngrams will be shifting throughout the period of 
interest. Instead we compute several groups of sentence features that are
specifically helpful for the query focused task. We divide our features
into two groups: static and dynamic features. Static features do not take into
account previous sentence selection decisions, while dynamic features
can be used information about current state of the update summary or stream
behaviour. \autoref{fig:strfeats} shows which feature groups were used 
in each of our two approaches. In general, incorporating dynamic features
into the learning-to-search approach is much easier.


\subsubsection{Static Features}
\paragraph{Simple Surface Features} 

The document stream that we will be operating over is incredibly noisy,
full of automatically extracted article text from raw web pages in 
dramatically different formatting. As a result, the underlying text is often
full of wep page headers, headlines to other stories, and irrelevant link text.We this group of basic features to help identify typical content bearing
sentences in the AP news style \citep{ap_style_guide}. These features 
include sentence length, the average number of capitalized words,
document position, sentence length in words, and the average number of 
named entities. Length and position features have been used previously in
other learning based models of sentence salience
\citep{kupiec1995trainable,conroy2001using}.

\paragraph{Query Features} To ensure the focus of the update summaries,
we employ query match features that count frequency of query term matches
in the sentence. We also do a simple query expansion using WordNet 
\citep{miller1995wordnet}
to find  synonyms, hypernyms, and hyponyms for the query event type and compute
a similar term match count with the expansion.
Queries in the Temporal Summarization data are also labeled with even type.
See \autoref{fig:eventtypes} for the list of query event types we used. 
For the \emph{earthquake} query type, for example, we find the following terms:
``quake'', ``temblor'', ``seism'', and ``aftershock''.


\begin{table}
\begin{tabular}{r | l}
\textbf{Event Type} & \textbf{Event Queries} \\
\hline
Storm & hurricane isaac, hurricane sandy, midwest derecho, typhoon bopha\\
Earthquake & guatemala earthquake  \\
Meteor Impact & russia meteor \\
Accident & buenos aires train crash,  pakistan factory fire\\
Riot & egyptian riots\\
Protest & bulgarian protests, egyptian protests \\
Hostages & in amenas hostage crisis \\
Shooting & colorado shooting, sikh temple shooting\\
Bombing & boston marathon bombing, hyderabad explosion \\
Conflict & konna battle \\
\end{tabular}
\caption{TREC Temporal Summarization query event types with example queries.} 
\label{fig:eventtypes}
\end{table}

\paragraph{Language Model Scores}

We rely on a pair of language models to assess the likelihood of observing
a given sentence in the document stream. The first language model is 
intended to be a generic news model, trained on New York Times and 
Associated Press sections of the Gigaword corpus \citep{graff2003english}. 
This model serves a similar purpose as the simple surface features in helping
to identify newswire-like sentences that are likely to contain informative
content.
The second model is query type specific and intended to focus the summarizer
on sentences from the same domain of the query. For each query type, we 
constructed an in-domain corpus of Wikipedia news articles belonging to
categories and pages related to the event type. The corpus is 
semi-automatically constructed: first a set of high-level categories are 
selected and then all pages belonging to those initial categories or
any sub-categories are automatically added. 
For example, the language
model for the event type ``earthquake'' is estimated
from Wikipedia pages under the category Category:Earthquakes.
For the actual summarization system features, we use the average token log
likelihood under each model as a feature.
We use the SRILM toolkit \citep{stolke2002srilm} to implement a 5-gram 
Kneser-Ney model \citep{kneser1995improved} for both the general and type
specific language models. 

\paragraph{Nugget Likelihood}
Using human judgements from 2014 Temporal Summarization we were able to obtain
example sentences that contained many of the nuggets in our corpus. We used
these judgements to train an ngram based classifier to predict a binary 
label $y$ for a sentence, with $y=1$ if it contained any event nugget and
$0$ otherwise. We used the probability of $y=1$ as an additional feature.

\paragraph{Single Document Summarization Features}
We used several unsupervised sentence ranking methods from the single 
document summarization literature to get several sentence rank features.
Inspired be the \textsc{SumBasic} summarizer \citep{nenkova2005impact}, we
compute the average and sum of unigram probability of each sentence using 
sentence's document unigram distribution. Inpsired by \citep{guo2013updating},
for each sentence we compute the average cosine distance to the remaining 
sentences in it's enclosing document. Finally, we compute the document
level centroid rank \citep{radev2000centroid} and LexRank 
\citep{erkan2004lexrank} for each sentence.

\subsubsection{Dynamic Features}
\paragraph{Geographic Relevance} 
The queries in the Temporal Summarization data are all grounded to physical
events in the world and we would like to exploit that. For example, while
there are sadly many reports of bombings or explosions at any given time 
around the world, when summarizing the query ``boston marathon bombing'' 
we are only really interested in news from a relatively proscribed 
geographic area, in this case, Boston, Massachusets. Implementing 
a geographic relevance signal is quite difficult in practice since 
we do not have ground truth location data for each event query, and some
events like hurricanes can be on the move. We opt instead to extract all
location mentions (using a named entity tagger) and get the lattitude
and longitude coordinates
of each mention using the Bing maps API \citep{bingmaps}. Location
mentions within the same hour are clustered, and the cluster centers are 
used as the canonical event locations for the query at that time. 
To get features for each sentence in the stream, we find the location of
the news article it came from and then compute average distance of the 
article to the query event locations.


%?The disasters
%?in our corpus are all phenomena that affect some
%?part of the world. Where possible, we would like
%?to capture a sentenceâ€™s proximity to the event, i.e.
%?when a sentence references a location, it should be
%?close to the area of the disaster.
%?There are two challenges to using geographic
%?features. First, we do not know where the event is,
%?and second, most sentences do not contain references
%?to a location. We address the first issue by
%?extracting all locations from documents relevant to
%?the event at the current hour and looking up their
%?latitude and longitude using a publicly available
%?geo-location service. Since the documents that are
%?at least somewhat relevant to the event, we assume
%?in aggregate the locations should give us a rough
%?area of interest. The locations are clustered and
%?we treat the resulting cluster centers as the event
%?locations for the current time.
%?The second issue arises from the fact that the
%?majority of sentences in our data do not contain
%?explicit references to locations, i.e. a sequence of
%?tokens tagged as location named entities. Our intuition
%?is that geographic relevance is important in
%?the disaster domain, and we would like to take ad


\paragraph{Temporal Relevance} As we track a query over time, the document
stream will ebb and flow as the underlying event unfolds. With hurricanes
for example, forecasters are usually watching a tropical storm as it grows
and predicitng where it might make landfall. As a result, news about a major
hurricane grows gradually, peaking after its main contact with land
and gradually diminishing. An eartquake on the otherhand gives no warning,
and there is a sudden spike in the query relevant document stream.
While we do not
do an extensive study of different event type shapes (which would certainly 
be very interesting), we would like to take advantage of spiking or bursty
activity in the docuemnt stream as a signal for our summarization system.
We do this by maintaining IDF values for each hour of the document stream.
We create 24 average TF-IDF features for each sentence as if it had occurred
in each of the previous 24 hours; large changes in any of the individual
features should be representative of a spike in the news.

\paragraph{Document Frequency} We also compute the hour-to-hour
percent change in document frequency of the stream. This
feature helps gauge breaking developments in an unfolding
event. As this feature is also heavily affected by the daily
news cycle (larger average document frequencies in the morning
and evening) we compute the 0-mean/unit-variance of this
feature using the training streams to find the mean and variance
for each hour of the day.



\paragraph{Stream Language Models}
We construct several simple unigram language models that we update with 
each new document that we see in the stream. Using these models 
we compute the sum, average, and maximum token likelihood
of the non-stop words
in the sentence. We compute similar quantities restricted to
the person, location, and organization named entities.

\paragraph{Update Similarity} The average and maximum cosine similarity
of the current sentence to all previous updates is computed
under both the tf-idf bag-of-words and latent vector
representation. We also include indicator features for when
the set of updates is empty (i.e. at the beginning of a run) and
when either similarity is 0.





Deep learning methods have become the defacto standard approach to many 
NLP problems, especially when there exists plentiful labeled data.
There has been a flurry of recent work on sentence extractive 
single document summarization of news using variety of neural network 
achitectures 
\citep{cheng2016neural,nallapati2016classify,nallapati2016summarunner,narayan2018ranking} 
thanks in part to the availibilty of a large corpus 
(approximately 300k) of CNN and Daily Mail articles with human written bullet 
point summaries \citep{hermann2015teaching}.
Comparing models and defining best practices for model design has become 
difficult as papers often propose fairly complicated models with a variety
of design choices, making it difficult to determine what choices actually
lead to the best performance. 

In this section, we describe completed experiments teasing out the importance
of different neural network designs for sentence level salience 
\cite{kedzie2018deep}. 
In particular, we experiment with several methods for encoding a sequence
of word embeddings into a sentence embedding, and then in turn, mapping
a sequence of sentence embeddings to sentence salience predictions. We 
introduce several simplifications to existing models in the literature, and
show their effectiveness on an SDS task across news, personal narratives,
workplace meetings, and medical journal article genres.

We also perform several diagnostic experiments on our deep learning based 
models and find several impedements to learning robust models of 
sentence salience. In particular, the sentence position implicitly 
encoded in the models dominates the learning signal. While sentence position
is certainly an important feature in news, not all domains or tasks are likely
have this the dominating feature; we would also like to be able to design
models that make their salience decisions primarily on lexical/topical content.

To that end, we propose a new deep learning based SDS model that directly 
estimates individual word level salience scores, and a simple sentence 
selection and margin loss framework for learning. In this model 
we augment the word embeddings (which only capture shallow lexical semantics)
with embeddings representing other word features. Our initial experiments
suggest that document frequency, and information theoretic accounts of 
surprisal (e.g. topic signatures) are more useful for the summarization task
than word window based embeddings.

We concluded this section with proposed domain adaptation experiments for
modifying the word importance model to work on a news MDS task. 
The MDS version of the model will have an importance score aggregation step
where word level scores accrue additional importance across documents 
using an attention mechanism.


%are the
%This
%work describes impedements to learning and word and sentence representations
%in deep learning models of extractive summarization, and led to 
%a recent publication \citep{kedzie2018deep}. Based on these limitations,
%we propose extensions to the word level representations and explicitly model
%word level salience scores as a means to performing sentence extractive
%summarization. While the finished and proposed work focuses on single document
%summarization, we also propose an extension of the word level salience
%estimation model that we hope will generalize to the multi-document
%summarization context.














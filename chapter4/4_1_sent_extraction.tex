
\newcommand{\sent}{s}
\newcommand{\Sents}{\mathcal{S}}
\newcommand{\doc}{D}
\newcommand{\lbl}{y}
\newcommand{\Labels}{Y}
\newcommand{\word}{w}
\newcommand{\vocab}{\mathcal{V}}
\newcommand{\extracts}{\mathcal{E}}
\newcommand{\budget}{c}
\newcommand{\encoder}{\textsc{Encoder}}
\newcommand{\extractor}{\textsc{Extractor}}
\newcommand{\embproj}{W}
\newcommand{\emb}{\omega}
\newcommand{\embsize}{n}
\newcommand{\sembsize}{m}
\newcommand{\semb}{h}
\newcommand{\rsemb}{\overrightarrow{\semb}}
\newcommand{\lsemb}{\overleftarrow{\semb}}
\newcommand{\gru}{\textsc{Gru}}
\newcommand{\rgru}{\overrightarrow{\gru}}
\newcommand{\lgru}{\overleftarrow{\gru}}
\newcommand{\cfeat}[2]{f_{#1}^{( #2 )}}
\newcommand{\relu}{\textsc{ReLU}}
\newcommand{\winsize}{k}
\newcommand{\winsizes}{K}
\newcommand{\fmapsize}{F}
\newcommand{\cfidx}{l}
\newcommand{\cact}{a^{(\cfidx,\winsize)}}
\newcommand{\cnnbias}{u^{(\winsize)}}
\newcommand{\cnnweight}{U^{(\winsize)}}
\newcommand{\cnnBiasSpace}{\mathbb{R}^{\fmapsize_\winsize}}
\newcommand{\cnnWeightSpace}{\mathbb{R}^{\winsize \times \fmapsize_\winsize \times \embsize}}


\subsection{Deep Learning Models of Sentence Extraction}

 Given the diversity of neural architectural choices, a best practices
for sentence extracive summarization has yet to emerge. In this section
we ask what architecture design choices matter for single document 
summarization across a variety of domains.

We begin by definining some terminology. A sentence is represented as an
sequence of words $\sent = \{\word_1, \word_2, \ldots, \word_{|\sent|} \}$
where each word is drawn from a fixed vocabulary $\vocab$ and $|\sent|$ is
the length of sentence $\sent$ in words. Similarly, a document $\doc = \{ 
\sent_1, \sent_2, \ldots, \sent_{|\doc|} \}$ is a sequence of sentences, where 
$|\doc|$ is the size of the document in sentences.  

We treat the sentence extractive summarization as sequence tagging
problem: given a document $\doc$, we want to assign an associated binary
tag sequence $\Labels = \{\lbl_1, \lbl_2, \ldots, \lbl_d \}
\in \{0,1\}^d$ such that the extract summary $\extracts = \{ 
\sent_i \in \doc : \lbl_i =1 \}$ implied by $\Labels$ is a suitable summary
of the document. Typically, it is assumed that size of the extract summary
$|\extracts| = \sum_{i=1}^{|\doc|} \mathbbm{1}\{\lbl_i =1 \} \ll |\doc|$.
It is also common to enforce a word budget $\budget$ such that
$\sum_{\sent \in \extracts} |\sent| \le \budget$.

A typical deep learning model will build up a hierarchical representation
of each sentence, starting at the word level, and then composing an arbitrarily
long sequence of word representations into a fixed length sentence 
representation.
First the individual words are 
projected to fixed length vectors, or word embeddings via a mapping
$\embproj : \vocab
\rightarrow \mathbb{R}^{\embsize}$. The sentence encoder network $\encoder :
\{\mathbb{R}^{\embsize}\}^* \rightarrow \mathbb{R}^{\sembsize}$ is then 
responsible for mapping word embbeding sequences to fixed length sentence 
embeddings. Finally, the sentence extractor network $\extractor : 
\{\mathbb{R}^\sembsize\}^* \rightarrow \{0, 1\}^*$ produces a label 
sequence $\Labels$.

We explore several choices of encoder and extractor architecture from the 
literature \citep{cheng2016neural,nallapati2016summarunner} as well as 
propose our own designs \citep{kedzie2018deep}. In the next sections,
we describe the three sentence encoder architectures (\textit{avg}, 
\textit{rnn}, and \textit{cnn}) followed by four extractor architectures 
(\textit{rnn}, \textit{seq2seq}, \textit{sr}, and \textit{cl}).

\subsubsection{Sentence Encoders}
\paragraph{Averaging} The simplest encoder simply averages a sentence's
associated word embeddings:
\[ \encoder_{avg}(\sent) = \frac{1}{|\sent|} \sum_{\word \in \sent} \embproj(\word). \]
Other than the word embeddings, this encoder involves no learned parameters,
and while it collapses word order, embedding averaging has consistently
been found competitive with more sophisticated sentence embedding techniques
\citep{iyyer2015deep,wieting2015towards,arora2016simple,wieting2017revisiting}.


\paragraph{Recurrent Neural Networks} The second encoder architecture
we experiment with is a recurrent neural network (RNN) over the word
embeddings. An RNN maintains an internal ``hidden'' state that is sequentially
updated upon observing each word embedding. In practice, we use a bidirectional
RNN with a gated recurrent unit (GRU) as the particular instantiation of 
the RNN cell \citep{cho2014learning}.
Under the RNN encoder, a sentence embedding for a sentence $\sent$ is defined 
as
\begin{align}
\encoder_{rnn}(\sent) = [\overrightarrow{\semb}_{|\sent|}, \overleftarrow{\semb}_{1} ] & \\
  \rsemb_0 = \mathbf{0};& \quad 
  \rsemb_i = \rgru\left(\embproj(\word_i), \rsemb_{i-1}\right) \\
  \lsemb_{|\sent| + 1} = \mathbf{0};& \quad 
  \lsemb_i = \lgru\left(\embproj(\word_i), \lsemb_{i+1}\right) 
\end{align}
where $[\cdot]$ is the concatenation operator; $\rsemb_i$ and $\lsemb_i$ are
hidden states of the forward and backward GRU cells respectively; and 
$\rgru, \lgru : \mathbb{R}^\embsize \times \mathbb{R}^\sembsize 
\rightarrow \mathbb{R}^\sembsize$ are the forward and backward GRU cell 
operations.
\cite{nallapati2016summarunner} use a bidirectional RNN for their sentence
encoder.

\paragraph{Convolutional Neural Networks}
Our final sentence encoder uses a convolutional neural network
(CNN) to encode important ngram windows into a fixed length vector. 
CNN's have grown increasingly popular in many NLP tasks 
as a computationally efficient substitute for RNN-based architectures
\citep{kim2014convolutional,lei2015molding,dauphin2017language}.
Our architecture largely follows \cite{kim2014convolutional}: we apply a 
series of one dimensional convoluions over a sentence's word embeddings
using varying width convolutions. For each convolutional window size 
$\winsize \in \winsizes \subset \mathbb{N}$, a convolutional filter creates 
a feature vector $\cfeat{}{\winsize} \in \mathbb{R}^{\fmapsize_\winsize}$ 
and the encoder output is the concatenation of the $|\winsizes|$ vectors. 
The set of filter window sizes $\winsizes$ and the number of feature maps
$\fmapsize_\winsize$ for each $\winsize \in \winsizes$ are 
model hyperparameters.
Formally, we define the CNN encoding of a sentence $\sent$ as 
\begin{align}
\encoder_{cnn}(\sent) & = \left[\cfeat{}{\winsize} : \winsize \in \winsizes \right]\\
\cfeat{\cfidx}{\winsize} &= 
     \max_{i \in 1,\dots, |\sent| - \winsize + 1} 
       \relu\left(\cact_i \right) \\
\cact_i &= \cnnbias_\cfidx
    + \sum^{\winsize}_{j=1} \cnnweight_{\cfidx,j} \cdot \embproj(\word_{i + j -1})
\end{align}
where $\cnnbias \in \cnnBiasSpace$ and $\cnnweight \in \cnnWeightSpace$
are learned convolutional filter weights and $\relu(x) = \max(0, x)$ 
is the rectified linear unit \citep{nair2010rectified}. \cite{cheng2016neural}
uses a CNN for their sentence encoder.

\subsubsection{Sentence Extractors}

A sentence extractor takes the encoder output, i.e. 
a sequence of sentence embeddings
$\encoder(\doc) =$ $\{\encoder(\sent_1), \ldots, \encoder(\sent_{|\doc|})\} =
\{\semb_1, \ldots, \semb_{|\doc|}\}$, and produces a sequence of labels
$\Labels = \{\lbl_1, \ldots, \lbl_{|\doc|}\}$. 
The sentence extractor is essentially a discriminative
classifier $p(\lbl_1,\ldots,\lbl_{|\doc|} | \semb_1,\ldots,\semb_{|\doc|})$.
Previous neural network approaches to sentence extraction have assumed
an auto-regressive model, leading to a semi-Markovian
factorization of the extractor probabilities
$p(\lbl_{1:n}|\semb)=\prod_{i=1}^{|\doc|} 
p(\lbl_i|\lbl_{<i},\semb)$,
where each prediction $\lbl_i$ is dependent on \emph{all}
previous $\lbl_j$ for
all $j < i$. We compare two such models proposed by \cite{cheng2016neural}
and \cite{nallapati2017summarunner}.
A simpler approach that does not allow interaction among the $\lbl_{1:n}$
is to
%\hal{a simpler approach (explain why simpler) is a fully factored representation 
  model $p(\lbl_{1:n}|\semb) = \prod_{i=1}^n p(y_i|h)$,
  which we explore in two proposed extractor models that we refer to as the RNN 
  and Seq2Seq extractors.
%Implementation details for all extractors are in \autoref{app:sentextractors}.


%\paragraph{Previously Proposed Sentence Extractors}
% We consider two recent state-of-the-art extractors.
\paragraph{Cheng \& Lapata Extractor} 
 The first extractor we consider, proposed by 
\citet{cheng2016neural}, %, which we refer to as the Cheng \& Lapata Extractor,
is built around a sequence-to-sequence model.
First, each sentence embedding\footnote{\citet{cheng2016neural} used an CNN sentence encoder with 
this extractor architecture; in this work we pair the Cheng \& Lapata extractor
with several different encoders.} is
fed into an encoder side RNN, with the final encoder state passed to the
first step of the decoder RNN. On the decoder side, the same sentence 
embeddings are fed as input to the decoder and decoder outputs are used to
predict each $\lbl_i$. The decoder input is weighted by the previous extraction
probability, inducing the dependence of $\lbl_i$ on $\lbl_{<i}$.
See \autoref{fig:extractors}.c for a graphical layout of the extractor.
%and \autoref{app:clextractor} for details.

%?, but are delayed by one step and 
%?weighted by their prediction probability, i.e. at decoder step $t$,
%?$p(\slabel[t-1]|\slabel[<t-1], \sentEmb[<t-1]) \cdot \sentEmb[t-1]$\hal{why did you switch from $i$ to $t$?}
%?is fed into the decoder\hal{i don't udnerstand what this means. what op is $\cdot$?}. The decoder output at step $t$ is concatenated 
%?to the encoder output step $t$ and fed through a multi-layer perceptron
%?with one hidden layer and sigmoid unit output computing the $t$-th
%?extraction probability $p(\slabel[t]|\slabel[<t], \sentEmb[<t])$. \textcolor{red}{See Figure 2.c. for a graphical view. Full model details are presented in ??}.
%?

\paragraph{SummaRunner Extractor}\citet{nallapati2017summarunner} proposed
a sentence extractor, which we refer to as the SummaRunner Extractor,
that factorizes the extraction probability into contributions 
from different sources.
First, a bidirectional RNN is run over the sentence embeddings\footnote{\citet{nallapati2017summarunner}
    use an RNN sentence encoder with 
this extractor architecture; in this work we pair the SummaRunner extractor
with different encoders. } and the output is
concatenated. A representation of the whole document is made by 
averaging the RNN output. A summary representation is also constructed 
by taking the sum of the previous RNN outputs weighted by their extraction
probabilities. Extraction predictions are made using 
the RNN output at the $i$-th step, the document representation, and 
$i$-th version of the summary representation, along with factors for 
sentence location in the document. The use of the iteratively constructed
summary representation creates a dependence of $\lbl_i$ on all $\lbl_{<i}$.
See \autoref{fig:extractors}.d for a graphical layout.
%

%\paragraph{Proposed Sentence Extractors}
%We propose two sentence extractor models that 
%make a stronger conditional independence 
%assumption $p(\slabel|\sentEmb)=\prod_{i=1}^\docSize p(\slabel[i]|\sentEmb)$,
%essentially making independent predictions conditioned on $\sentEmb$.


\paragraph{$\extractor_{rnn}$}
    Our first proposed model is a very simple bidirectional
RNN based tagging model. As in the RNN sentence encoder we use a GRU cell.
The forward and backward outputs of each sentence are passed through a 
multi-layer perceptron with a logsitic sigmoid output 
to predict the probability
of extracting each sentence. 
See \autoref{fig:extractors}.a for a graphical layout.
%and \autoref{app:rnnextractor} for details.


\newcommand{\rExtHidden}{\overrightarrow{h}}
\newcommand{\lExtHidden}{\overrightarrow{h}}
\newcommand{\docSize}{|\doc|}
\newcommand{\logits}{o}

\begin{align}
    \rExtHidden_0 = \mathbf{0};&\quad   \rExtHidden_i = \rgru(\semb_i, \rExtHidden_{i-1}) \\
    \lExtHidden_{\docSize + 1} = \mathbf{0};&\quad    \lExtHidden_i = \lgru(\semb_i, \lExtHidden_{i+1}) \\
   \logits_i &= \relu\left(U \cdot [\rExtHidden_i; \lExtHidden_i] + u \right)\\
    p(\lbl_i=1|\encoder(\doc)) &= \sigma\left(V\cdot \logits_i + v  \right)
\end{align}
where $\rgru$ and $\lgru$ indicate the 
forward and backward GRUs respectively, and each have separate learned 
parameters; $U, V$ and $u, v$ are learned weight and bias parameters.
The hidden layer size of the GRU is 300 for each direction and the MLP hidden layer
size is 100. Dropout is applied to the GRUs and to $a_i$.





\paragraph{$\extractor_{s2s}$} One shortcoming of the RNN extractor is that long range
information from one end of the document may not easily be able to affect 
extraction probabilities of sentences at the other end. 
Our second proposed model, the $\extractor_{s2s}$ mitigates this problem with an 
attention 
mechanism commonly
used for neural machine translation \cite{bahdanau2014neural} and 
abstractive summarization \cite{see2017get}. 
The sentence embeddings are first
encoded by a bidirectional $\gru$. A separate decoder $\gru$ transforms each 
sentence into a query vector which attends to the encoder output. The
attention weighted encoder output and the decoder $\gru$ output are concatenated
and fed into a multi-layer perceptron to compute the extraction probability.
See \autoref{fig:extractors}.b for a graphical layout.




\subsection{Experiments}

 We are interested in two questions. The first, more pragmatic question, is
 what are the best configuration of encoder/extractor architectures?
 We answer this question by evaluating ROUGE recall and METEOR \citep{meteor}
 performance across our six collected datasets. We perform the standard
 stochastic gradient descent \cite{sgd} based optimization (using the Adam
 update \cite{adam}) of the weighted negative log likehood 
 \[ \mathcal{L}(\theta) = -\sum_{\Sents, \Labels \in \mathcal{D}} 
            \sum_i \omega(\lbl_i) 
        \log p(\lbl_i|\lbl_{<i}, \Sents; \theta) \]
        where $\theta$ are model parameters and $\omega(\lbl_i)$ upweights
        the positive labels to account for the imbalanced label distribution.

 We lack human reference extract labels for our datasets and so we obtain
 said lable sequences heuristically, by finding a label sequence $\Labels^*$
 by greedily optimizing ROUGE-1 recall with respect to the human reference
 abstracts.

 The second question, is more diagnostic in nature: what signals
 in the data are driving model learning?
 We perform several experiments to find answers. 
 We hypothesize that the lexical semantics encoded at the word embedding
 level will be important to subsequent sentence representations, and
 perform a comparison on learning with and with out fine tuning of the 
 embeddings. In both cases, embeddings are initialized with Glove
 embeddings pretrained on Wikipedia and Gigaword \citep{glove}.
 
 
 We also hypothesize that certain classes of words will be more important 
 to identifying salient content than others. We perform word ablation 
 experiments where we alternately remove nouns, verbs, adjectives \& adverbs,
 and function words from the sentence encoder input and compare performance 
 to the non-ablated system. We expect that the nouns will be more important
 to content selection. 


 Our final experiments attempt to tease out the effect of structural features 
 from the lexical. In this experiment, we shuffle the sentence order at 
 training time. In this setup, we obfuscate features about which content 
 was introduced in the article first, an important and well known bias in 
 news domain \cite{leadbias}. 


 \subsection{Results}


  

The results of our main experiment comparing 
the different extractors/encoders are shown in 
Table~\ref{tab:results}.
Overall, we find no major advantage when using the CNN and RNN sentence
encoders over the averaging encoder. The best performing encoder/extractor pair either 
uses the averaging 
encoder (five out of six datasets) or the differences 
are not statistically significant. %When only comparing within the 
%same extractor choice,  the averaging encoder is the better choice
%in 14 of 20 cases. 
%\hal{i wonder if it would be worth adding another ``average performance metric'' column to \autoref{tab:results}.
%  i'm thinking have ``Average $\Delta$-Best'' meaning how far (on average across the datasets) is this setting from the best setting available on that dataset.
%  so since the best numbers are: 25.56, 35.85, 23.11, 13.65, 5.63
%  and the first row numbers are: 25.42, 34.67, 22.65, 11.37, 5.50
%  then the deltas are:            0.14,  1.18,  0.46,  2.28, 0.13
%  the the average delta is 0.84 (assuming my math is right)
%  there's an argument to do multiplicative, in which case
%  the multipliers for first row:  0.99,  0.97,  0.98,  0.83, 0.97
%  and the average is 0.95
%  either way this gives a quick way to make comparisons between rows. you could do the same for the other tables too.}

  %\hal{in some of the tables you list R-2 as headers even though all the numbers are R-2. just put that in the caption.}


When looking at extractors, the Seq2Seq extractor is either part of 
the best performing system (three out of six datasets) or is not 
statistically distinguishable from the best extractor. 

Overall, on the news and medical journal domains, the differences are 
quite small with the 
differences between worst and best systems on the CNN/DM dataset 
spanning only .56 of a ROUGE point. While there is more performance variability
 in the Reddit and AMI data, there is less distinction among systems: 
 no differences are significant on Reddit
and every extractor has at least one configuration that is indistinguishable
from the best system on the AMI corpus. This is probably due to the small test
size of these datasets.
%\hal{this is probably at least partially because of test set size. maybe mention this.}





%?\textcolor{red}{Overall we find that the \modelTwoBF~extractor achieves the 
%?best ROUGE scores on three out of four domains (STILL RUNNING ON AMI AND PUBMED). 
%?However, most
%?differences are not signficant. (Need to discuss stat sig and how to show it).}
%?On the larger CNN-DailyMail dataset, especially, 
%?differences are quite smail across all extractor/encoder pairs.
%?The \baselineOneBF~extractor achieves the best performance on the DUC 2002
%?dataset. It is disappointing that the \baselineOneBF~and \baselineTwoBF~based 
%?models do not gain any apparent advantage in conditioning on previous 
%?sentence selection decisions; this result suggests the need to improve
%?the representation of the summary as it is being constructed iteratively.
%?
%?\textbf{Choice of Encoder} We also find there to be no major advantage 
%?between the different sentence encoders. \textcolor{red}{In most cases,
%?there is no statistical significance between the averaging encoder and either
%?the RNN or CNN encoders.} 

%The lack of differentiation amongst the different encoders concerning; one
%would assume learning with the appropriate structure would be helpful.
%The results of next 




\paragraph{Word Embedding Learning}
 Given that learning a sentence encoder (averaging has no learned parameters)
 does not yield significant improvement, it is natural to consider whether
 learning word embeddings is also necessary. 
 In \autoref{tab:embeddings} we compare the performance of different extractors
 using the averaging encoder, when the word embeddings are held fixed or 
 learned during training. In both cases, word embeddings are initialized with
 GloVe embeddings trained on a combination of Gigaword and Wikipedia.
% \hal{TRAINED ON WHAT? JUST THE DEFAULT ONES?}
 When learning embeddings, words occurring 
 fewer than three times in the training data are mapped to an unknown
 token (with learned embedding).
 
% shows ROUGE recall
%when using fixed or updated word embeddings. 
 In all but one case,
fixed embeddings are as good or better than the learned embeddings.
This is a somewhat surprising finding on the CNN/DM data since it is reasonably
large, and learning embeddings should give the models more
flexibility to identify important word features.\footnote{The AMI corpus is an exception here where learning \emph{does} lead to small
performance boosts, however, only in the Seq2Seq extractor is this diference 
significant; it is quite possible that this is an artifact of the very small
test set size.}
%\hal{why is it surprising?} \textcolor{green}{[[CK: Because learning embeddings should give the model more capacity to represent important patterns]]}
This suggests that we cannot extract much generalizable learning signal 
from the content other than what is already present from initialization. 
Even on PubMed, where the language is quite different from the news/Wikipedia
articles the GloVe embeddings were trained on, learning leads to 
significantly worse results.

%The language of this corpus is quite different from the 
%data that the GloVe embeddings were trained on and so it makes sense 
%that  there would be more benefit to learning word representations; one
%explanation for only seeing modest improvements is purely the small size
%of the test dataset which has only 20 training meetings.

%textcolor{red}{(NOTE TO CK -- expect learning to help on pubmed)}. \hal{yes, the dataset size is certainly an issue here. probably worth pointing this out. also when you learned the embeddings, did you initialize to pretrained embeddings? did you regularize toward them?}


\paragraph{POS Tag Ablation}
It is also not well explored what word features are being used by the encoders.
To understand which classes of words were most important we ran an ablation
study, selectively removing nouns, verbs 
(including participles and auxiliaries), adjectives \& adverbs, and 
function words (adpositions, determiners, conjunctions).
%Additionally, we ran ablation experiments
%using part-of-speech (POS) tags. \hal{this needs to be justified. why is this experiment interesting?}
All datasets were automatically tagged using
the spaCy part-of-speech (POS)
tagger\footnote{https://github.com/explosion/spaCy}.   
%\kathy{I'm still curious what would happen if you separately removed all conjunction tags and later remaining POS.}
%We experimented with selectively removing 
%\begin{itemize}
%    \item nouns (NOUN and PROPN tags), 
%    \item verbs (VERB, PART, and AUX tags), 
%    \item adjectives/adverbs (ADJ and ADV tags), 
%    \item numerical expressions (NUM and SYM tags), and 
%    \item miscellaneous words (ADP, CONJ, CCONJ, DET, INTJ, and SCONJ tags)
%\end{itemize}
%from each sentece. 
The embeddings of removed words were replaced with a zero vector,
preserving the order and position of the non-ablated words in the sentence.
Ablations were performed on training, validation, and test partitions,
using the RNN extractor with averaging encoder.
\autoref{tab:ablations} shows the results of the POS
tag ablation experiments. 
While removing any word class from the representation generally hurts 
performance (with statistical significance), on the news domains,
the absolute values of the differences are quite small 
(.18 on CNN/DM, .41 on NYT, .3 on DUC) suggesting that the model's predictions
are not overly dependent on any particular word types.
On the non-news datasets, the ablations have a larger effect 
(max differences are 1.89 on Reddit, 2.56 on AMI, and 1.3 on PubMed).
Removing nouns leads to the largest drop on AMI and PubMed.
Removing adjectives and adverbs leads to the largest drop on Reddit,
suggesting the intensifiers and descriptive words are useful for 
identifying important content in personal narratives.
Curiously, 
removing the function word POS class yields a significant improvement
on DUC 2002 and AMI.


%The newswire domain does not appear to be sensative
%to these ablations; this suggests that the models are still able to identify
%the lead section of the document with the remaining word classes \textcolor{red}{(Verify this with histogram analysis)}. 
%The Reddit domain, which is not lead biased, is significantly effected.
%Notably, removing adjectives and adverbs results in a 1.8 point drop 
%in ROUGE-2 recall. 


\textbf{Document Shuffling} Sentence position is a well known and 
powerful feature for news summarization \cite{hong2014improving}, owing 
to the intentional lead bias in the news article writing\footnote{\url{https://en.wikipedia.org/wiki/Inverted_pyramid_(journalism)}}; it also explains the difficulty in beating
the lead baseline for single-document summarization 
\cite{nenkova2005automatic,rau:1999}.
In examining the generated summaries, we found
most of the selected sentences in the news domain came from the lead paragraph
%\hal{i feel like there must be citations to dig up here from like the 90s about lead summarization in news... it's also an intentional bias: maybe the right thing is to cite a style guide from a newsppaer that says to write this way}
of the document. This is despite the fact that there is a long tail of 
sentence extractions from later in the document in the ground truth extract 
summaries (31\%, 28.3\%, and 11.4\% of DUC, CNN/DM, and NYT training extract labels come 
from the second half of the document). 
%\hal{can you be more specific? like give some stats? what \%age come from first quarter of doc and what \%age from last half or something}. 
Because this lead bias is so strong, it is questionable whether
the models are learning to identify important content or just find the start
of the document. We conduct a sentence order experiment where 
each document's sentences are randomly shuffled during training. We then
%KM - I think below should be shuffled. I changed.
%CK - models are trained on shuffled data but evaluated on in order models.
%evaluate each model performance on the unshuffled test data, comparing to 
evaluate each model performance on the unshuffled test data, comparing to 
the model trained on unshuffled data; if the models trained on shuffled data
drop in performance, then this indicates the lead bias is the relevant factor.
%in learning content selection.

\autoref{tab:shuffle} shows the results
of the shuffling experiments. 
The news domains and PubMed suffer a significant drop in performance 
when the document order is shuffled. By comparison, there is no significant difference between the shuffled and in-order models on 
the Reddit domain, and shuffling actually improves performance on AMI.
%\hal{what about in the cross-domain setting?} 
This suggest that position 
is being learned by the models in the news/journal article domain even when 
the model has no explicit position features, and that this feature is more 
important than either content or function words.












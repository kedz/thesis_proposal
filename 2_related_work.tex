\section{Related Work}


The complete range of methods and task variations covered by the summarization
literature are too numerous to describe completely here. In the space 
available, we give an overview of unsupervised and supervised learning
approaches to estimating word and sentence importance. 
We also give some background on the variety
of summarization tasks, and the manner in which they are evaluated.


\subsection{Unsupervised Word Importance}


The most obvious signal available for estimating word importance is word 
frequency, i.e. frequent word occurences (modulo stopwords) are likely 
to be related the important topics of the document or document collection 
\cite{topic_sigs}, and it is the chief ingredient in all of the following
word importance estimators.
Term weighting schemes from information retrieval
have frequently been used, most commonly tf-idf 
\cite{centroid,maybe_conroy,find_others}. 
Various probabilistic weightings have also been considered, e.g. the
observed document probability \cite{freq_sum} or the ratio of background
corpus probabiltiy to document probability \cite{topic_sigs}.
Information theoretic transformations of word frequency like the document
level term entropy
have also been found to be useful \cite{klsum,reference_less_summarization}.
Latent semantic analyis (LSA), latent dirichlet allocation (LDA), and
other matrix factorization schemes have been used to find a low dimensional
``concept'' representation for each word, the sum of the concept contributions
constituting the word importance score \cite{multiling_stuff}.
Finally, various methods of transforming word co-occurences into a word graph
have been used in conjunction with unsupervised graph ranking algorithms
\cite{lexrank,conroy_multiling,zhao_2009}.


\subsection{Unsupervised Sentence Importance}

 Graph based ranking that has been used for word importance has similarly 
been used to estimate sentence importance, where the graph consists of 
sentence vertices and edge weights are computed using pairwise sentence 
similarity \cite{textrank}. Another popular method is the centroid method
\cite{centroid} where an average vector representation for a document set is 
constructed and sentences are ranked by their similarity to the centroid object.
Sentence clustering methods have also been applied. In all these methods, 
sentences are tpyically represented as term vectors where the weights for 
each term item are determined by one of the methods mentioned in word importance
estimation methods. 


\subsection{Supervised Word Importance}

The growing availibilty of large text collections with summaries has also
encouraged the development of many supervised methods for learning the word 
importance 
directly from the data. \cite{Regsum} use many of the unsuperivsed word  
importance measures, (e.g. frequency, entropy, document position) to estimate
the probabiltiy of a word occuring in a human abstract. Learning-to-rank
has also been explored in a variety of contexts. For example in single
document summarization, \cite{lapata} used convolutional neural networks
learn rankings and while \cite{guo,macreadie} use a feature based model
to rank sentences in a streaming summarization task.
Word importance weights have also been learned in combinatorial optimization
based approaches to summarization; in several integer linear program (ILP)
formulations of the summarization problem unigram and bigram weights 
are learned \cite{gillick,martens,berkely}, in addition to learning based
submodular optimization approaches \cite{submod,svm}.




\subsection{Unsupervised Methods}



 


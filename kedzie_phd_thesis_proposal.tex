\documentclass[12pt]{article}
\usepackage{wrapfig}
\usepackage{adjustbox}
\usepackage{array}

\newcolumntype{R}[2]{%
    >{\adjustbox{angle=#1,lap=\width-(#2)}\bgroup}%
    l%
    <{\egroup}%
}
\newcommand*\rot[2]{\multicolumn{1}{R{#1}{#2}}}

\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{dsfont}
\usepackage{url}
\usepackage[a4paper,left=1in,top=1in,right=1in,bottom=1in,nohead]{geometry}
\usepackage[round]{natbib}
\usepackage{multirow}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{subcaption}


\newcommand{\query}{q}
\newcommand{\strsent}{s}
\newcommand{\strsents}{\mathcal{S}}
\newcommand{\nugget}{n}
\newcommand{\extract}{a}
\newcommand{\salience}{y}
\newcommand{\Salience}{Y}
\newcommand{\similarity}{k}
\newcommand{\Similarity}{K}
\newcommand{\updates}{\mathcal{U}}
\newcommand{\nuggets}{\mathcal{N}}
\newcommand{\exemplars}{\mathcal{E}}



\newcommand{\tfidf}{tf-idf }





\newcommand{\poi}{T}

\title{\textbf{What to Write and How to Write It}\\
       \textit{\large Modeling Content Selection and Generation for Text Summarization}\\
       ~\\
       ~\\\large\textbf{Thesis Proposal}}
\author{~\\
        \textbf{Chris Kedzie}\\
        Department of Computer Science\\
        Columbia University\\
        \url{kedzie@cs.columbia.edu}}

\date{December $10^{\textrm{th}}$, 2018}

\begin{document}

\pagenumbering{gobble}
\maketitle

\pagebreak

\renewcommand{\thepage}{\roman{page}}
\setcounter{page}{1}

\begin{abstract}
Automatic text summarization is a long standing NLP problem that has recently 
seen an uptick in interest due to flexible, high capacity deep learning based 
sequence-to-sequence transduction models. 
While many researchers are turning to complex neural networks to perform
end-to-end summary generation, we argue that this unnecessarily obscures
the underlying sub-tasks for summarization. Instead, we propose an alternative
research agenda, focusing on two key summarization subtasks: 
\textit{i)} estimating the general importance of text content for summary 
inclusion, and \textit{ii)} generating text that is faithful, a notion we 
formally define, to said important content. With respect to problem \textit{i)}
we propose several novel methods for working in both low context, streaming
news scenarios, as well as, standard single document summarization settings,
including feature-based and deep learning models of content importance.
On the latter problem, we study the utility of using extractive 
summarization algorithms as a preprocessing stage for a sequence-to-sequence 
based abstractive summarizer, and finally introduce a novel algorithm for
generating text that is faithful, i.e. respects prior beliefs about structured 
knowledge in a database, in an effort to provide stronger guarantees 
about summary reliability.
\end{abstract}

\pagebreak

\tableofcontents
\newpage

\renewcommand{\thepage}{\arabic{page}}
\setcounter{page}{1}
\input{1_introduction.tex}
\input{2_related_work.tex}
\input{3_streaming_summarization}
\input{4_single_document_summarization.tex}


\bibliographystyle{plainnat}
\bibliography{cites}

\end{document}

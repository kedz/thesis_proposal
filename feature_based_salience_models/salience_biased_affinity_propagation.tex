\subsection{Model 1: Salience-Biased Affinity Propagation Clustering}  
\label{sec:sapmodel}


%  \input{feature_based_salience_models/figures/3_fig_sap_algo.tex}

  Identifying potential updates from the document stream is hard in part
  because we may not have enough context to use word frequencies as a 
  reliable proxy for salience. 
  Our first proposed method accounts for this
  by
  processesing the stream in hourly batches, i.e. we collect all the
  sentences from the last hour and then decide which sentences if any to
  add to the update summary. The trade-off we make is that fast breaking
  events may not immediately be covered by the summarizer. The benefit
  is that we can now decompose our salience estimation into two components,
  a regression estimate of salience for each sentence individually,
  and a set of pairwise factors that describe how representative a sentence 
  is of the other batch items. 


%  The unary factor is estimated using a regression model with sentence 
%  features described in \autoref{sec:features}.
%   
%  The pair-wise factor can be considered a measure
%  of how representative a sentence is of the other batch items.
%  We use the semantic similarity method of \cite{guo2012simple} to compute
%  similarity

  We use an exemplar-based clustering algorithm
  called Affinity Propagation (AP) \citep{frey2007clustering}
  to systematically combine these unary and pairwise factors into a joint 
  selection of cluster centers, called exemplars, that we consider as 
  candidate updates.
  A sentence that has high overall similarity to
  the other sentences in the batch is likely to be an exemplar and
  more so if it's salience estimation is high relative to the batch.



  
%  that combines
%  sentence level salience predictions with pairwise sentence similarities
%  to identify a set of exemplar sentences that would make for good 
%  additions to the updates summary. The pairwise similarity factors work 
%  as our replacement for the ngram-frequency based signal we would normally 
%  use in traditional MDS;   
  
  Finally, given a set of exemplar sentences, we remove any
  sentences whose similarity to any previous update is above a threshold.
  The remaining exemplars are added to the update summary.
  See \cite{kedzie2015predicting} for the full algorithm.
 % The final algorithm is presented in \autoref{alg:ts_sap_algo}.



%  We describe our process for computing salience and similarity in
 %the next sections, before describing the AP clustering algorithm.



%?  We next describe the 
%?
%?
%?
%?  In this model, we process the document stream in hourly batches, 
%?  first predicting the salience of the individual sentences and then 
%?  using affinity propagation (AP) to select a set of exemplar sentences.
%?  Exemplar sentences that have predicted salience above a threshold 
%?  and are below a similarity threshold to previously select sentences are
%?  then emitted to the user as an update summary.





  \subsubsection{Salience Estimation}

  















  %Given a sentence $\strsent \in \strsents$ we would like to predict it's 
  %salience with respect to a query $\query$. 
  When we were developing this salience 
  model,
  we did not have access to many human judgements of query or nugget relevance,
  and so we relied on an automatic measure of sentence salience computable
 directly from the nugget and sentence texts.
Given a query $\query$, sentence text $\strsent$, and a query's nugget texts 
$\nugget \in \mathcal{N}(q)$, we define
the sentence salience $\salience$ as
\[ 
  \salience = \max_{\nugget\in \nuggets(\query)} \similarity{\strsent}{\nugget}
\]
where $\similarity{\cdot}{\cdot}$ is the cosine similarity of a
low-dimensional representation of the sentence and nugget text.
We used the weighted matrix factorization method of \cite{guo2012simple}
which projects a text's sparse high-dimensional bag-of-words representation
into a dense, low-dimensional vector. 
%We also use this method to compute 
%sentence similarity in the AP clustering algorithm.


%  Given the feature representation of a sentence, we want to predict how
%confident we are that it contains one or more nuggets. While we have
%many sentences in our corpus, we did not have may sentence level judgements
%about the nuggets that they contained. 
%Lacking these gold annotations, we instead get noisy salience annotations 
%using the maximum similarity of the nugget texts to the sentence texts.
%Formally, given a query $q$, sentence text $s$, and query's nugget texts $n \in \mathcal{N}(q)$
%the sentence salience $y$ is 
%using the following equation:
%\[ y = \max_{n \in  \mathcal{N}(q)} \operatorname{sim}(s, n)\]
%where $\operatorname{sim}$ is a semantic similarity measure (in practice
%we used the weight matrix factorization method of \cite{wmtf}).

We use a Gaussian process regression model \citep{rasmussen2004gaussian}
to predict $\salience$ from $\strsent$ 
\emph{without} knowledge of the nuggets. 
For each feature group $g$ in \autoref{sec:features} we create a seperate
radial basis function (RBF) kernel for each feature group and the 
 actual Gaussian process regressor is specified by 
the summation kernel 
$k(\strsent, \strsent^\prime) = \sum_g k_g(\strsent, \strsent^\prime)$
of the individual group kernels.


%\[
%  k_g(\strsent, \strsent^\prime) =\sigma^2_g
%     \exp\left(
% - \frac{|| \feats_g(\strsent) - \feats_g(\strsent^\prime)  ||^2}{2\ell_g}\right)
%\]
%where $\fdef{\feats_g}{\strsents}{\Rn{d_g}}$ maps sentences to a
%$d_g$-dimensional vector of features for group $g$ and $\sigma^2_g, \ell_g$
%are kernel parameters. 
%The actual Gaussian process regressor is specified by 
%the summation kernel 
%$k(\strsent, \strsent^\prime) = \sum_g k_g(\strsent, \strsent^\prime)$
%of the individual group kernels.

%We fit a seperate regressor for each query $\query$ in our training set
%by sampling a set of 1000 sentences 
%$\sentvec = (\strsent_1, \ldots, \strsent_{1000})$ 
%drawn uniformly from $\strsents(\query)$ and computing their 
%corresponding salience scores 
%$\saliences = (\salience_1, \ldots, \salience_{1000})$. We fit the 
%kernel parameters with gradient descent to maximize the likelihood of
%$\saliences$.
%At prediction time for a test query, we 
%use the average prediction of each training query regressor to obtain
%a salience estimate $\hat{\salience}$ for each sentence in the test stream.



%?
%? When given a sentence $\strsent$ from a test document stream 
%? $\strsents(\query_*)$, we compute the average posterior predictive mean 
%? $\E_{\gp_\query}[\salience|\strsent, \sentvec^(\query),\saliences^{(\query)}]$
%? \\
%?
%?
%?
%?To obtain a prediction for a sentence $\strsent \in \strsents(\query_*)$
%?for a test query $\query_*$,
%?
%?
%?
%?
%?
%?
%?
%?to obtain prediction for an out of sample sentence $\strsent^\prime$, 
%?we compute the posterior predictive
%?mean
%?
%?
%?
%?
%?We specify 
%?the regressor $\saliences \sim \gp(\mathbf{0}, K(\sentvec, \sentvec))$
%?~\\~\\
%?for the list of feature groups used in the salience regressor.
%?We use seperate radial basis function (RBF) kernels for each 
%?feature group, and use the sum of all the kernels as final
%?kernel matrix for fitting the model.
%?We fit a separate regressor for each query in our dataset using 1000
%?randomly sampled sentences from each query's associated relevant document 
%?stream. 
%?At prediction time for a specific query, we hold out that query's salience
%?model, and use the average prediction of the remaining models to obtain
%?a salience estimate $\hat{\salience}$ for sentences in the stream.

\subsubsection{Salience-biased Affinity Propagation}


    Affinity Propagation (AP) is a factor-graph based clustering
    method that simultaneously selects the most representative data points 
    to be cluster centers, referred to as \emph{exemplars},  and maps 
    the remaining data points to exactly one of those exemplars 
    \citep{frey2007clustering}. 
    The exemplar mappings determine the clusters.
    AP has a number of nice properties for extractive summarization.
    First, as an exemplar based clustering method, the cluster centers
    are guaranteed to be an actual sentence observed in the input, and not 
    an abstract mathematical object like a mean in the input 
    feature space.
    Additionally, the number of clusters that result is adaptive and 
    based on the energy of the factor graph configurations; as we have to 
    summarize many hours of stream data of varying density, we can avoid
    having to heuristically propose a number of cluster centers that works
    across a broad range of stream conditions.


 Typically, all inputs to AP clustering are equally likey apriori to
 be selected as exemplars.
 %case the exemplar self-affinity factors are 
%simply 
%set to a constant indicating all points are equally likely apriori to serve
%as exemplars. 
 In our case, we have some prior beliefs about the importance of 
a given datapoint as expressed by our salience predictions $\predSalience_i$.
By replacing the self-affinity potentials with our salience predictions
and the pairwise potentials with our semantic similarity function from the 
previous section %, and setting $\discSet$ to be a batch of sentences 
%$\strsents_t \subset \strsents$,
we obtain the salience-biased affinity propagation model.
When we estimate a sentence to be more salient it is more likely apriori
to form a cluster center. When that sentence is also highly similar to other 
sentences in the batch it collects support from those sentences, further
increasing it's likelihood of being assigned as an exemplar.
 


%    Given a set of $d$ datapoints $\discSet = \{x_1, \ldots, x_d\}$, 
%    AP finds a set of
%    exemplar datapoints $\exemplars \subset \discSet$ and an assignment
%    $A : \discSet \rightarrow\exemplars$ of the 
%    remaining points to one of the exemplars. Exemplars map to themselves,
%    i.e. for $x \in \exemplars,$ $A(x) = x$. The configuration of exemplars
%    and exemplar assignments is represented as a factor graph, 
%    with the following objective:
%    
%    \[ \mathcal{L}(\mathcal{E}, A) 
%        = \exp\left[
%            \sum_{i=1}^d \operatorname{affinity}\Big(x_i, A(x_i)\Big) + 
%          \log \delta_i \right] \]
%    where $\delta_i = \begin{cases} 0 & \textrm{if $A(x_i) \neq x_i$ and $\exists j: j\neq i \wedge A(x_j)=x_i$} \\
%1 & \textrm{otherwise}\end{cases}$ is a constraint that enforces all clusters
% have one and only one exemplar and $\operatorname{affinity}:\discSet 
% \times \discSet \rightarrow \mathbb{R} $
% is a function describing the pairwise similarity of points in 
% $\discSet$.
% The max-product message passing algorithm,
% a form of loopy belief propagation, can be used to find a configuration
% that approximately maximizes the objective function \citep{dueck2009affinity}.
%
% We can refactor the first term in the exponent to be 
%\[ \sum_{x_i \in \exemplars} \operatorname{affinity}(x_i, x_i) +
% \sum_{x_j \in \mathcal{X} \setminus \exemplars} \operatorname{affinity}(x_j, A(x_j))
% \]
%where the first term is the sum of the affinity factors of the exemplars 
%to themselves,
%and the second term consists of  pairwise factors between each non-exemplar 
%datapoint 
%and it's exemplar. In the typical case the exemplar self-affinity factors are 
%simply 
%set to a constant indicating all points are equally likely apriori to serve
%as exemplars. In our case, we have some prior beliefs about the importance of 
%a given datapoint as expressed by our salience predictions $\predSaliences_i$.
%Replacing the self-affinity potentials with our salience predictions, 
%the pairwise potentials with our semantic similarity function from the 
%previous section, and setting $\discSet$ to be a batch of sentences 
%$\strsents_t \subset \strsents$,
%we arrive at the salience-biased affinity propagation
%objective:
%
% \[ \mathcal{L}(\exemplars, A) = \exp\left[
%%    \sum_{\substack{\strsent \in \exemplars \\ \hat{\salience} = \operatorname{salience}(\strsent, \query)}} \hat{\salience}  
%         \sum_{\strsent \in \exemplars} \predSaliences_{\strsent}
%     + \sum_{\strsent \in \strsents_t \setminus \exemplars} \similarity{\strsent}{A(\strsent)} + \sum_{i=1}^{|\strsents_t|} \log\delta_i \right].
%\] 
%When we estimate a sentence to be more salient it is more likely apriori
%to form a cluster center. When that sentence is also highly similar to other 
%sentences in the batch it collects support from those sentences, further
%increasing it's likelihood of being assigned as an exemplar.
 



%~\\
%~\\
%    the net similarity objective
%    \[ \mathcal{L}(X, \mathcal{E}) = 
%    \sum_{i \in \mathcal{E}} \operatorname{salience}(x_i) + \sum_{i \in\mathcal{E}} \sum_{j:e_j = i}\operatorname{similarity}(x_i, x_j)  \]
%    where $\operatorname{salience}$ and $\operatorname{similarity}$ 
%    are unary and pairwise factors that express the degree to which
%    $x_i$ is apriori likey to be an examplar and that $x_i$ is a suitable
%    representative for $x_j$. 
%    $\mathcal{L}$ is optimized using and iterative message passing 
%    algorithm. In the naive setting, $\operatorname{salience}$ is uniform
%    across all $x_i$, i.e. every data point is equally likely to be an
%    exemplar, and exemplar assignment is purely determined by the 
%    pairwise similarity factors.
%    
%    In our present summarization scenario, we have a strong prior belief
%    about suitability of a particular sentence to be an examplar which is
%    represented by the salience predictions $\hat{y}_i$.
% Our system processes hourly batches of sentences 
% $\mathcal{S}_{\textrm{batch}} = \{s_1, s_2, \ldots \}$ by first predicting
% their corresponding saliences $\hat{Y} = \{\hat{y}_1, \hat{y}_2, \ldots\}$.
% We then use AP clustering to find an assignment of exemplar sentences
% that maximizes the following objective:
% \[ \mathcal{L}(\mathcal{S}_{\textrm{batch}}, \mathcal{E}) = 
%    \sum_{i \in \mathcal{E}} \hat{y}_i + \sum_{i \in\mathcal{E}} \sum_{j:e_j = i}\operatorname{similarity}(s_i, s_j)  \]
%    where $\operatorname{similarity}$ is the same WTMF method used in the 
%    noisy salience annotation above. We refer to this method as biased-AP
%    clustering, since the exemplar selection is now biased by our prior
%    beliefs about the sentence's importance to the query.
%
%
%    After the examplars are selected, we perform one final filtering of
%    exemplars, discarding sentences that have a $y_i$ below $\lambda_{sal}$
%    or that have a maximum similarity to any previous updates above
%    $\lambda_{sim}$, where the $\lambda$'s are preset thresholds. 
%    Exemplars that survive filtering are selected for the update summary.

\subsubsection{Data}

    \input{feature_based_salience_models/figures/3_tab_aps_rouge.tex}
  The document streams come from the news portion of the 2014 TREC
KBA Stream Corpus \citep{frank2012building}, which contains hourly crawls
of the web covering a roughly two year span from 2011 to 2013.
Event queries and their nuggets were taken from the data prepared for 
the 2013 and 2014 TREC TS tracks. This data
contained 25 events and their query strings, time period of interest, and
event type. 
Additionally, each event was associated with anywhere from 50 to several
hundred timestamped nugget texts. 
%Each event query was significant 
%enough to have a Wikipedia page. Event nuggets were taken manually
%extracted from the corresponding Wikipedia entry, using the earliest
%revision that contained the nugget to obtain it's timestamp.
For details on the creation of this dataset see \cite{aslam2014trec,aslam2015trec}.
From the larger KBA Stream Corpus we created event specific document 
streams by filtering out any documents that did occur in the period
of interest and contain all the query words of the corresponding event.  



    \subsubsection{Experiments}

    \input{feature_based_salience_models/figures/3_fig_aps_rouge_time.tex}

Of the 25 events in the TREC TS data, 24 are
covered by the news portion of the TREC KBA
Stream Corpus. From these 24, we set aside
three events to use as a development set. All
system salience and similarity threshold parameters
are tuned on the development set to maximize
\rougeN{2} \fmeasure{} scores.
We train a salience model for each event using
1000 sentences randomly sampled from the
event's document stream.
We perform a leave-one-out evaluation of each
event. At test time, we predict a sentenceâ€™s
salience using the average predictions of the 23
other models.



    Since we lacked gold judgements about what sentences contain which nuggets
    we perform an automatic evaluation using \rouge{} \citep{lin2004rouge}. We 
    create reference summaries for each query by concatenating all 
    of it's nugget texts.
    Since there are no fixed summary lengths, and depending on the severity
    of the event, the reference summaries can vary greatly in length.
    To account for this, we report \rouge{} recall, precision and \fmeasure{}
    score.
We also approximate the manual evaluation of the official TREC TS track
    by automatically mapping sentences to nuggets if their semantic similarity
    is a above a threshold. 
    We report $\expectedGain,$ $\comp,$ and their \fmeasure{} score
    across a sweep of similarity threshold 
    values from zero to one, with values closer to one a more conservative
    estimate of performance.     


    We refer to our approach as \sap{} and compare to 
    several 
    baselines.
    The first is our full system but with uniform salience scores, i.e.
    the default AP clustering algorithm. We refer to
    this method as \ap.
    The second is to rank all sentences in each batch in order of decreasing
    predicted
    salience, and sequentially adding each sentence to the update summary, 
    omitting
    any sentences with similarity to previous updates above a threshold.
    This method is referred to as \ranksal{} for rank by salience.
    Finally, we compare against another clustering algorithm,
    hierarchical agglomerative clustering.
    In this method, sentences are first clustered, 
    and then centers are determined by the sentence
    with  highest  cosine  similarity  to  the  cluster
    mean. 
    Sentences are added to the update summary in time order, 
    removing sentences that are highly similarity to previous updates in the
    same manor as the \ranksal{} method. We refer to this method as 
    \hac.



\input{feature_based_salience_models/figures/3_fig_aps_autots.tex}

    \subsubsection{Results}

\autoref{tab:aps_rouge}  shows  our  results  for  system  output
samples against the full summary of nuggets using \rouge. 
This improvement is statistically significant  for  all  \ngram{}
precision,  recall,  and \fmeasure{} scores at the
$\alpha=.01$
level using the Wilcoxon
signed-rank test.
\sap{}
maintains    its    performance
above  the  baselines  over  time  as  well.
\autoref{fig:3_aps_rouge_time}  shows  the  \rougeN{1}  scores  over  
time.
We  show  the  difference  in  unigram  precision
(bigram  precision  is  not  shown  but  it  follows a
similar  curve).
Within  the  initial  days  of  the
event,  \sap{}
is  able  to  take  the  lead
over  the  over  systems  in  ngram  precision.   The
\sap{}
model is better able to find salient
updates earlier on; for the disaster domain, this is
an especially important quality of the model.
Moreover, the \sap{} recall is not diminished by the high 
precision and remains competitive with \ap. 
Over time \sap's recall also begins to pull away, 
while the other models start to suffer from topic drift.


\autoref{fig:3_aps_autots} shows the $\expectedGain$ and $\comp$
across a range
of  similarity  thresholds,  where  thresholds  closer
to 1 are more conservative estimates. The ranking
of the systems remains constant across the sweep
with \sap{}
beating all baseline systems.
Predicting salience in general is helpful for keeping a summary on topic as 
the  \ranksal{}  approach out
performs  the  clustering  only  approaches  on  $\expectedGain$.
When looking at the $\comp$ of the
summaries \ap{} outperforms \sap.  
The compromise  encoded  in  the  \sap{}
objective function, between being representative and
being salient, is seen clearly here where the performance of the \sap{}
methods is lower
bounded by the salience focused  \ranksal{}  system and
upper bounded by the clustering only AP system.
Overall, \sap{}
achieves the best balance
of these two metrics.



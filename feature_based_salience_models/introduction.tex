Learning models of sentence salience in summarization is often difficult
because summarization datasets are typically small (a few hundred training
examples in many cases) and learning lexical 
feature weights directly is likely to run into issues of coverage on the test
set. To get around this problem, we rely on heavily lexicalized components 
trainable on unlabeled data to produce scores that serve as unlexicalized 
features in our salience models. The canonical example of this is to use 
a language model trained on in-domain but non-summarization data to 
obtain average token perplexity scores for a sentence; the salience model
only sees the average perplexity and is blissfully unaware of any lexical 
features. 
In this section we 
describe our features in the context of stream summarization task, but the 
features are themselves fairly generic and likely obtainable in other task
settings.

A second but important issue is that salience judgments do not occur
in isolation. As we add content to the summary, the salience
of our remaining inputs is likely change based on redundancy and other factors.
Unfortunately, adding summary/sentence interaction features introduces an element of 
exploration to training the model for now various summary configuration 
and candidate sentence pairs must be considered.

We propose two model based solutions to this. 
The first model, Salience-biased Affinity Propagation (SAP) 
\citep{kedzie2015predicting}, combines 
independent sentence level predictions into a clustering algorithm,
Affinity Propagation (AP) \citep{frey2007clustering}, that also 
considers sentence similarity to jointly select salient and 
non-redundant sentences for inclusion in the summary.

Our second model, Learning-to-Summarize (L2S) \citep{kedzie2016real}, allows 
us to freely incorporate summary/sentence interaction
features, as we train the salience model using the learning-to-search regime
\cite{daume2009search,chang2015learning}
where learning takes place using different exploration policies. Using 
this method we can learn a salience model that makes good individual sentence
selection choices (i.e. good local decisions) that also correlate to a good 
final summary (i.e. good global decisions). 

In the next subsections, we will describe the stream summarization task
in detail, before moving on to the sentence features, and finally the models
and their evaluations.


